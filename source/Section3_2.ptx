<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="section3_2-Bayes">
	<!-- Most of this section from Schmid book ch 5 and 6 -->
	<title>Conditional Probability, Independence, and Bayes' Rule</title>
	<idx>Bayes' Rule</idx><idx>Independence</idx>
	<subsection  xml:id="subsec3_2_1-conditional-probability"><title>Conditional probability</title><idx>Conditional Probability</idx>
		<introduction><p>Anil Maheshwari has two children. We are told that one of them is a
				boy. What is the probability that the other child is also a boy?
				Most people will say that this probability is <m>1/2</m>. We will show below
				that this is not the correct answer. </p>

			<p>Since Anil has two children, the sample space is
				<me>S = \{ (b,b) , (b,g) , (g,b) , (g,g) \} , </me>

				where, for example, <m>(b,g)</m> indicates that the youngest child is a
				boy and the oldest child is a girl. We assume a uniform probability
				function, so that each outcome has a probability of <m>1/4</m>. </p>

			<p>We are given the additional information that one of the two children
				is a boy, or, to be more precise, that at least one of the two
				children is a boy. This means that the actual sample space is not
				<m>S</m>, but
				<me> \{ (b,b) , (b,g) , (g,b) \} .</me>

				When asking for the probability that the other child is also a boy,
				we are really asking for the probability that both children are boys.
				Since there is only one possibility (out of three) for both children
				to be boys, it follows that this probability is equal to <m>1/3</m>. </p>

			<p>This is an example of a <em>conditional probability</em>: We are asking
				for the probability of an event (both children are boys) given that
				another event (at least one of the two children is a boy) occurs.</p>
		</introduction>

		<definition  xml:id="def-conditional-probability">
			<title>Conditional Probability</title><idx>conditional probability</idx>
			<statement><p>
					Let <m>(S,P)</m> be a probability space and let <m>A</m> and <m>B</m> be two events
					with <m>P(B) > 0</m>. The <em>conditional probability</em> <m>P(A \mid B)</m>,
					pronounced as <q>the probability of <m>A</m> given <m>B</m></q>, is defined to be
					<me>P(A \mid B) = \frac{P(A \cap B)}{P(B)} . </me>
					</p></statement></definition>

		<example xml:id="ex-anils-children"><title>Probability of a second boy</title>
			<p>Returning to Anil's two children, we saw that the sample space is
				<me>S = \{ (b,b) , (b,g) , (g,b) , (g,g) \}  </me>

				and we assumed a uniform probability function. The events we considered
				are
				<me>A= \text{both children are boys} </me>

				and
				<me>B= \text{at least one of the two children is a boy} </me>

				and we wanted to know <m>P(A \mid B)</m>. Writing <m>A</m> and <m>B</m> as
				subsets of the sample space <m>S</m>, we get
				<me>A = \{ (b,b) \}  </me>

				and
				<me>B = \{ (b,b) , (b,g) , (g,b) \} .  </me>

				Using <xref ref="def-conditional-probability"/>, it follows that
				<me>P(A \mid B) = \frac{P(A \cap B)}{P(B)}
					= \frac{P(A)}{P(B)}
					= \frac{|A|/|S|}{|B|/|S|}
					= \frac{1/4}{3/4}
					= 1/3 , </me>

				which is the same answer as we got before. </p></example>


		<example xml:id="ex-condl-prob-dice-roll"><title>Probability of rolling a three given an odd number is rolled</title>
			<p>Assume we roll a fair die, i.e., we choose an element uniformly at
				random from the sample space
				<me> S = \{1,2,3,4,5,6\} .</me>

				Consider the events
				<me>A=\text{the result is } 3 </me>

				and
				<me>B=\text{the result is an odd integer.} </me></p>


			<p>What is the conditional probability <m>P(A \mid B)</m>? To determine this
				probability, we assume that event <m>B</m> occurs, i.e., the roll of
				the die resulted in one of <m>1</m>, <m>3</m>, and <m>5</m>. Given that event <m>B</m>
				occurs, event <m>A</m> occurs in one out of these three possibilities.
				Thus, <m>P(A \mid B)</m> should be equal to <m>1/3</m>. We are going to
				verify that this is indeed the answer we get when using
				<xref ref="def-conditional-probability"/>: Since
				<me>A = \{ 3 \} </me>

				and
				<me>B = \{ 1,3,5 \} ,</me>

				we have
				<me>P(A \mid B) = \frac{P(A \cap B)}{P(B)}
					= \frac{P(A)}{P(B)}
					= \frac{|A|/|S|}{|B|/|S|}
					= \frac{1/6}{3/6}
					= 1/3 . </me>
			</p>

			<p>Let us now consider the conditional probability <m>P(B \mid A)</m>.
				Thus, we are given that event <m>A</m> occurs, i.e., the roll of the die
				resulted in <m>3</m>. Since <m>3</m> is an odd integer, event <m>B</m> is
				guaranteed to occur. Therefore, <m>P(B \mid A)</m> should be equal to <m>1</m>.
				Again, we are going to verify that this is indeed the answer we get
				when using <xref ref="def-conditional-probability"/>:
				<me>P(B \mid A) = \frac{P(B \cap A)}{P(A)}
					= \frac{P(A)}{P(A)}
					= 1 . </me>

				This shows that, in general, <m>P(A \mid B)</m> is not equal to
				<m>P(B \mid A)</m>. Observe that this is not surprising. (Do you see why?)</p></example>

		<theorem xml:id="thm-sum-of-conditional-complements">
			<title>Sum of the conditional probabilities of complements</title>
			<statement><p>
					Let <m>(S,P)</m> be a probability space and let <m>A</m> and <m>B</m> be two events
					with <m>P(B)>0</m>. Then
					<me> P(A \mid B) + P \left( \overline{A} \mid B \right) = 1 . </me>

					</p></statement>

			<proof><p>
					By definition, we have
					<me>\begin{array}{cc}
						P(A \mid B) + P \left(\overline{A} \mid B\right) &amp;=
						\frac{P(A \cap B)}{P(B)} +
						\frac{P\left(\overline{A} \cap B\right)}{P(B)} \\
						&amp;= \frac{P(A \cap B) +
						P\left(\overline{A} \cap B\right)}{P(B)} .
						\end{array} </me>
					Since the events <m>A \cap B</m> and <m>\overline{A} \cap B</m> are disjoint,
					we have, by <xref ref="thm-probability-of-sequence-of-disjoint"/>,
					<me> P(A \cap B) + P\left(\overline{A} \cap B\right) =
						P\left( (A \cap B) \cup \left(\overline{A} \cap B\right)
						\right) .</me>

					By drawing a Venn diagram, you will see that
					<me>(A \cap B) \cup \left(\overline{A} \cap B\right) = B ,</me>

					implying that
					<me>P(A \cap B) + P\left(\overline{A} \cap B\right) = P(B) .</me>

					We conclude that
					<me>P(A \mid B) + P\left(\overline{A} \mid B\right) =
						\frac{P(B)}{P(B)} = 1 .</me></p> </proof></theorem>

		<theorem xml:id="thm-law-of-total-probability">
			<title>The Law of Total Probability</title>
			<statement><p>
					Let <m>(S,P)</m> be a probability space and let <m>A</m> be an event.
					Assume that <m>B_1,B_2,\ldots,B_n</m> is a sequence of events such that
					<ol>
						<li><m>P \left( B_i \right) > 0</m> for all <m>i</m> with <m>1 \leq i \leq n</m>, </li>
						<li> the events <m>B_1,B_2,\ldots,B_n</m> are pairwise disjoint, and  </li>
						<li><m>\bigcup_{i=1}^n B_i = S</m>. </li></ol>

					Then
					<me>P(A) = \sum_{i=1}^n P \left( A \mid B_i \right) \cdot
						P \left( B_i \right) .</me>

					</p></statement>
			<proof><p>The assumptions imply that
				<me>\begin{array}{cc}
					A &amp;= A \cap S \\
					&amp;= A \cap \left( \bigcup_{i=1}^n B_i \right) \\
					&amp;= \bigcup_{i=1}^n \left( A \cap B_i \right) .
					\end{array} </me>
				Since the events <m>A \cap B_1 , A \cap B_2 , \ldots , A \cap B_n</m>
				are pairwise disjoint, it follows from Lemma~\ref{lemSumPr} that
				<me>\begin{array}{cc}
					P(A) &amp;= P \left( \bigcup_{i=1}^n \left( A \cap B_i \right)
					\right) \\
					&amp;=  \sum_{i=1}^n P \left( A \cap B_i \right) .
					\end{array} </me>
				The theorem follows by observing that, from <xref ref="def-conditional-probability"/>,
				<me>P \left( A \cap B_i \right) =
					P \left( A \mid B_i \right) \cdot P \left( B_i \right) .</me></p>
				</proof></theorem>


	</subsection>


	<subsection  xml:id="subsec3_2_2-independent-events"><title>Independent Events</title><idx>Independent events</idx>
		<introduction><p>
				Consider two events <m>A</m> and <m>B</m> in a sample space <m>S</m>. In this section,
				we will define the notion of these two events being <q>independent</q>.
				Intuitively, this should express that (i) the probability that event <m>A</m>
				occurs does not depend on whether or not event <m>B</m> occurs, and
				(ii) the probability that event <m>B</m> occurs does not depend on whether
				or not event <m>A</m> occurs. Thus, if we assume that <m>P(A)>0</m> and
				<m>P(B)>0</m>, then (i) <m>P(A)</m> should be equal to the conditional
				probability <m>P(A \mid B)</m>, and (ii) <m>P(B)</m> should be equal to the
				conditional probability <m>P(B \mid A)</m>. As we will show below, the
				following definition exactly captures this. </p></introduction>

		<definition  xml:id="def-independent-events">
			<title>Independent Events</title><idx>Independent Events</idx>
			<statement><p>
					Let <m>(S,P)</m> be a probability space and let <m>A</m> and <m>B</m> be two events.
					We say that <m>A</m> and <m>B</m> are <em>independent</em> if
					<me> P(A \cap B) = P(A) \cdot P(B) .
						</me></p>

				<p>In this definition, it is not assumed that <m>P(A)>0</m> and <m>P(B)>0</m>.
					If <m>P(B) > 0</m>, then
					<me> P(A \mid B) = \frac{P(A \cap B)}{P(B)} ,
					</me>
					and <m>A</m> and <m>B</m> are independent if and only if
					<me> P(A \mid B) = P(A) .
					</me>
					Similarly, if <m>P(A)>0</m>, then <m>A</m> and <m>B</m> are independent if and only if
					<me> P(B \mid A) = P(B) .
						</me> </p></statement></definition>

		<example xml:id="ex-rolling-two-dice-independent"><title>Independence of Rolling Two Dice</title>
			<p>
				Assume we roll a red die and a blue die; thus, the sample space is
				<me> S = \{ (i,j) : 1 \leq i \leq 6 , 1 \leq j \leq 6 \} ,
				</me>
				where <m>i</m> is the result of the red die and <m>j</m> is the result of the
				blue die. We assume a uniform probability function. Thus, each
				outcome has a probability of <m>1/36</m>.</p>

			<p>	Let <m>D_1</m> denote the result of the red die and let <m>D_2</m> denote the
				result of the blue die. Consider the events
				<me> A: D_1+D_2=7
				</me>
				and
				<me>B: D_1=4.
				</me>
				Are these events independent?
				<ul>
					<li><p> Since
					<me> A = \{ (1,6) , (2,5) , (3,4) , (4,3) , (5,2) , (6,1) \} ,
					</me>
					we have <m>P(A) = 6/36 = 1/6</m>.</p></li>
					<li> <p>Since
					<me> B = \{ (4,1) , (4,2) , (4,3) , (4,4) , (4,5) , (4,6) \} ,
					</me>
					we have <m>P(B) = 6/36 = 1/6</m>.</p></li>
			<li><p> Since
					<me> A \cap B = \{ (4,3) \} ,
					</me>
					we have <m>P(A \cap B) = 1/36</m>.</p></li>
					<li> <p>It follows that <m>P(A \cap B) = P(A) \cdot P(B)</m> and we
						conclude that <m>A</m> and <m>B</m> are independent.</p></li>
					</ul></p>

			<p>As an exercise, you should verify that the events
				<me> A_2: D_1+D_2=11
				</me>
				and
				<me> B_2: D_1=5
				</me>
				are not independent.</p>

			<p>Now consider the two events
				<me> A_3: D_1+D_2=4
				</me>
				and
				<me> B_3: D_1=4.
				</me>
				Since <m>A_3 \cap B_3 = \emptyset</m>, we have
				<me> P \left( A_3 \cap B_3 \right) = P(\emptyset) = 0 .
				</me>
				On the other hand, <m>P \left( A_3 \right) = 1/12</m> and
				<m>P \left( B_3 \right) = 1/6</m>. Thus,
				<me> P \left( A_3 \cap B_3 \right) \neq
					P \left( A_3 \right) \cdot P \left( B_3 \right)
				</me>
				and the events <m>A_3</m> and <m>B_3</m> are not independent. This is not
				surprising: If we know that <m>B_3</m> occurs, then <m>A_3</m> cannot not occur. Thus, the event <m>B_3</m> has
				an effect on the probability that the event <m>A_3</m> occurs.
				</p></example>


		<p>	Consider two events <m>A</m> and <m>B</m> in a sample space <m>S</m>. If these events
			are independent, then the probability that <m>A</m> occurs does not depend
			on whether or not <m>B</m> occurs. Since whether or not <m>B</m> occurs is the
			same as whether the complement <m>\overline{B}</m> does not or does occur,
			it should not be a surprise that the events <m>A</m> and <m>\overline{B}</m> are
			independent as well.
		</p>
		<theorem xml:id="thm-independence-of-complements">
			<title>Independence of Complements</title>
			<statement><p>
					Let <m>(S,P)</m> be a probability space and let <m>A</m> and <m>B</m> be two events.
					If <m>A</m> and <m>B</m> are independent, then <m>A</m> and <m>\overline{B}</m> are also
					independent.
					</p></statement>
			<proof><p>
					To prove that <m>A</m> and <m>\overline{B}</m> are independent, we have to show
					that
					<me>  P \left( A \cap \overline{B} \right) =
						P(A) \cdot P \left( \overline{B} \right) .
					</me>
					Using <xref ref="thm-probability-of-complements"/>, this is equivalent to showing
					that
					<me>
						P \left( A \cap \overline{B} \right) =
						P(A) \cdot \left( 1 - P(B) \right) .</me>
					Since the events <m>A \cap B</m> and <m>A \cap \overline{B}</m> are disjoint and
					<me> A = \left( A \cap B \right) \cup
						\left( A \cap \overline{B} \right) ,
					</me>
					it follows from <xref ref="thm-probability-of-sequence-of-disjoint"/> that
					<me> P(A) = P( A \cap B ) +
						P \left( A \cap \overline{B} \right) .
					</me>
					Since <m>A</m> and <m>B</m> are independent, we have
					<me> P(A \cap B) = P(A) \cdot P(B) .
					</me>
					It follows that
					<me> P(A) = P(A) \cdot P(B) +
						P \left( A \cap \overline{B} \right) ,
					</me>
					which is equivalent to <me>
						P \left( A \cap \overline{B} \right) =
						P(A) \cdot \left( 1 - P(B) \right) .\Box</me>.</p></proof>
		</theorem>


		<p>	We have defined the notion of two events being independent. The
			following definition generalizes this in two ways to sequences of
			events:</p>

		<definition  xml:id="def-pairwise-mutually-independent">
			<title>Pairwise and Independent Events</title><idx>Pairwise and Mutually Independent Events</idx>
			<statement><p>
					Let <m>(S,P)</m> be a probability space, let <m>n\geq 2</m>, and let
					<m>A_1,A_2,\ldots,A_n</m> be a sequence of events.
					<ol>
						<li> <p>We say that this sequence is <em>pairwise independent</em> if
								for any two distinct indices <m>i</m> and <m>j</m>, the events <m>A_i</m> and
								<m>A_j</m> are independent, i.e.,
								<me> P \left( A_i \cap A_j \right) =
									P \left( A_i \right) \cdot P \left( A_j \right) .
									</me></p></li>
						<li> <p>We say that this sequence is <em>mutually independent</em> if
								for all <m>k</m> with <m>2 \leq k \leq n</m> and all indices
								<m>i_1 &lt;i_2 &lt;\ldots &lt;i_k</m>,
								<me> P \left( A_{i_1} \cap A_{i_2} \cap \cdots \cap A_{i_k}
									\right) =
									P \left( A_{i_1} \right) \cdot P \left( A_{i_2} \right)
									\cdots P \left( A_{i_k} \right) .
									</me></p></li>
					</ol>
					</p></statement></definition>

		<p>				Thus, in order to show that the sequence <m>A_1,A_2,\ldots,A_n</m> is
			pairwise independent, we have to verify <m>n \choose 2</m> equalities.
			On the other hand, to show that this sequence is mutually independent,
			we have to verify <m>\sum_{k=2}^n {n \choose k} = 2^n - 1 - n</m>
			equalities.</p>

		<p>				For example, if we want to prove that the sequence <m>A,B,C</m> of three
			events is mutually independent, then we have to show that
			<me> P(A \cap B) = P(A) \cdot P(B) ,
			</me>
			<me> P(A \cap C) = P(A) \cdot P(C) ,
			</me>
			<me> P(B \cap C) = P(B) \cdot P(C) ,
			</me>
			and
			<me> P(A \cap B \cap C) = P(A) \cdot P(B) \cdot P(C) .
				</me></p>

		<example xml:id="ex-pairwise-but-not-mutual-independence"><title>Pairwise but not Mutually Independent Coin Flips</title>
			<p>
				Consider flipping a coin three times and assume
				that the result is a uniformly random element from the sample space
				<me> S = \{ HHH , HHT , HTH , THH , HTT , THT , TTH , TTT \} ,
				</me>
				where, e.g., <m>HHT</m> indicates that the first two flips result in heads
				and the third flip results in tails. Define the events
				<me> A = \text{"flips 1 and 2 have the same outcome"},
				</me>
				<me> B = \text{"flips 2 and 3 have the same outcome"},
				</me>
				and
				<me> C = \text{"flips 1 and 3 have the same outcome".}
				</me>
				If we write these events as subsets of the sample space, then we get
				<me> A = \{ HHH , HHT , TTH , TTT \} ,
				</me>
				<me> B = \{ HHH , THH , HTT , TTT \} ,
				</me>
				and
				<me> C = \{ HHH , HTH , THT , TTT \} .
				</me>
				It follows that
				<me> \begin{array}{ccccccc}
					P(A) &amp; = &amp; |A|/|S| &amp; = &amp; 4/8 &amp; = &amp; 1/2 , \\
					P(B) &amp; = &amp; |B|/|S| &amp; = &amp; 4/8 &amp; = &amp; 1/2 , \\
					P(C) &amp; = &amp; |C|/|S| &amp; = &amp; 4/8 &amp; = &amp; 1/2 , \\
					P(A \cap B) &amp; = &amp; |A \cap B|/|S| &amp; = &amp; 2/8 &amp; = &amp; 1/4 , \\
					P(A \cap C) &amp; = &amp; |A \cap C|/|S| &amp; = &amp; 2/8 &amp; = &amp; 1/4 , \\
					P(B \cap C) &amp; = &amp; |B \cap C|/|S| &amp; = &amp; 2/8 &amp; = &amp; 1/4 .
					\end{array}
				</me>
				Thus, the sequence <m>A,B,C</m> is pairwise independent. Since
				<me> A \cap B \cap C = \{ HHH , TTT \} ,
				</me>
				we have
				<me> P(A \cap B \cap C) = |A \cap B \cap C|/|S| = 2/8 = 1/4 .
				</me>
				Thus,
				<me> P(A \cap B \cap C) \neq P(A) \cdot P(B) \cdot P(C)
				</me>
				and, therefore, the sequence <m>A,B,C</m> is <em>not</em> mutually independent.
				Of course, this is not surprising: If both events <m>A</m> and <m>B</m> occur,
				then event <m>C</m> also occurs.
				</p></example>
	</subsection>
	<subsection xml:id="subsec3_2_3-random-variables"><title>Random Variables</title><idx>Random Variables</idx>
		<introduction>
			<p><blockquote><p>A random variable is neither random nor variable.</p></blockquote></p>
			<p>
				We have already seen random variables in <xref ref="section3_1-probability"/>,
				even though we did not use that term there. For example, in
				<xref ref="ex-flipping-a-coin"/>, we rolled a die twice and were
				interested in the sum of the results of these two rolls. In other words,
				we did an <q>experiment</q> (rolling a die twice) and asked for a function
				of the outcome (the sum of the results of the two rolls).</p></introduction>

		<definition  xml:id="def-random-variable">
			<title>Random Variable</title><idx>Random Variable</idx>
			<statement><p>
					Let <m>S</m> be a sample space. A <em>random variable</em> on the sample space
					<m>S</m> is a function <m>X : S \rightarrow \mathbb{R}</m>.
					</p></statement></definition>
		<p>
			In the example given above, the sample space is
			<me> S = \{ (i,j) : 1 \leq i \leq 6 , 1 \leq j \leq 6 \}
			</me>
			and the random variable is the function <m>X : S \rightarrow \mathbb{R}</m> defined
			by
			<me> X(i,j) = i+j
			</me>
			for all <m>(i,j)</m> in <m>S</m>.</p>

		<p>Note that the term <q>random variable</q> is misleading: A random variable
			is <em>not</em> random, but a function that assigns, to every outcome
			<m>\omega</m> in the sample space <m>S</m>, a real number <m>X(\omega)</m>. Also, a
			random variable is <em>not</em> a variable, but a function.</p>

		<example xml:id="ex-random-variables-three-coins"><title>Flipping Three Coins with Random Variables</title>
			<p>
				Assume we flip three coins. The sample space is
				<me> S = \{ HHH , HHT , HTH , HTT , THH , THT , TTH , TTT \} ,
				</me>
				where, e.g., <m>TTH</m> indicates that the first two coins come up tails and
				the third coin comes up heads.</p>

			<p>Let <m>X : S \rightarrow \mathbb{R}</m> be the random variable that maps any
				outcome (i.e., any element of <m>S</m>) to the number of heads in the
				outcome. Thus,
				<me> \begin{array}{lcc}
					X(HHH) &amp; = &amp; 3 ,  \\
					X(HHT) &amp; = &amp; 2 , \\
					X(HTH) &amp; = &amp; 2 , \\
					X(HTT) &amp; = &amp; 1 , \\
					X(THH) &amp; = &amp; 2 , \\
					X(THT) &amp; = &amp; 1 , \\
					X(TTH) &amp; = &amp; 1 , \\
					X(TTT) &amp; = &amp; 0 .
					\end{array}
				</me>
				If we define the random variable <m>Y</m> to be the function
				<m>Y: S \rightarrow \mathbb{R}</m> that
				<ul>
				<li>maps an outcome to <m>1</m> if all three coins come up heads or all
				three coins come up tails, and</li>
				<li>maps an outcome to <m>0</m> in all other cases,</li>
			</ul>
				then we have
				<me> \begin{array}{lcc}
					Y(HHH) &amp; = &amp; 1  ,  \\
					Y(HHT) &amp; = &amp; 0  , \\
					Y(HTH) &amp; = &amp; 0  ,  \\
					Y(HTT) &amp; = &amp; 0  ,  \\
					Y(THH) &amp; = &amp; 0  ,  \\
					Y(THT) &amp; = &amp; 0  ,  \\
					Y(TTH) &amp; = &amp; 0  ,  \\
					Y(TTT) &amp; = &amp; 1  .
					\end{array}
					</me></p>

<p>			Since a random variable is a function <m>X : S \rightarrow \mathbb{R}</m>, it
			maps any outcome <m>\omega</m> to a real number <m>X(\omega)</m>. Usually, we
			just write <m>X</m> instead of <m>X(\omega)</m>. Thus, for any outcome
			in the sample space <m>S</m>, we denote the value of the random variable,
			for this outcome, by <m>X</m>. In the example above, we flip three coins
			and write
			<me> X = \text{ the number of heads}
			</me>
			and
			<me> Y =
				\left\{ \begin{array}{ll}
				1 &amp; \text{if all flips have the same outcome,} \\
				0 &amp; \text{otherwise.}
				\end{array}
				\right.
			</me>
			</p></example>


			<p>Random variables give rise to events in a natural way. In the
			three-coin example, <q><m>X=0</m></q> corresponds to the event <m>\{TTT\}</m>,
			whereas <q><m>X=2</m></q> corresponds to the event <m>\{HHT,HTH,THH\}</m>.
			The table below gives some values of the random variables <m>X</m> and <m>Y</m>,
			together with the corresponding events.
			<me>\begin{array}{|c|l|}  \hline
			\text{value} &amp; \text{event} \\
			\hline
			X=0 &amp; \{TTT\}  \\
			X=1 &amp; \{HTT,THT,TTH\}  \\
			X=2 &amp; \{HHT,HTH,THH\}  \\
			X=3 &amp; \{HHH\}  \\
			X=4 &amp; \emptyset  \\
			\hline
			Y=0 &amp; \{ HHT , HTH , HTT , THH , THT , TTH \}  \\
			Y=1 &amp; \{HHH,TTT\} \\
			Y=2 &amp; \emptyset  \\
			\hline
			\end{array}</me></p>


	<p>Thus, the event <q><m>X=x</m></q> corresponds to the set of all outcomes that
			are mapped, by the function <m>X</m>, to the value <m>x</m>:</p>

		<definition  xml:id="def-X-equals-x">
			<title><m>X = x</m></title><idx>X = x</idx>
			<statement><p>
			Let <m>S</m> be a sample space and let <m>X : S \rightarrow \mathbb{R}</m> be a random
			variable. For any real number <m>x</m>, we define <q><m>X=x</m></q> to be the
			event
			<me> \{ \omega \in S : X(\omega) = x \} .
			</me></p></statement></definition>

			<example xml:id="ex-prob-random-variables-three-coins"><title>Probabilities of Flipping Three Coins with Random Variables</title>
				<p>
			Let us return to the example in which we flip three coins. Assume that
			the coins are fair and the three flips are mutually independent. Consider
			again the corresponding random variables <m>X</m> and <m>Y</m>. It should be clear
			how we determine, for example, the probability that <m>X</m> is equal to <m>0</m>,
			which we will write as <m>P(X=0)</m>. Using our interpretation of
			<q><m>X=0</m></q> as being the event <m>\{TTT\}</m>, we get
<me>			\begin{array}{rl}
			P(X=0) &amp; = P(TTT) \\
			&amp; = 1/8 .
			\end{array}</me>
			Similarly, we get
			<me>\begin{array}{rl}
			P(X=1) &amp; = P(\{HTT,THT,TTH\})  \\
			&amp; =  3/8 , \\
			P(X=2) &amp; =  P(\{HHT,HTH,THH\}) \\
			&amp; =  3/8 , \\
			P(X=3) &amp; =  P(\{HHH\}) \\
			&amp; =  1/8 , \\
			P(X=4) &amp; =  P(\emptyset) \\
			&amp; =  0 ,  \\
			P(Y=0) &amp; =  P(\{ HHT , HTH , HTT , THH , THT , TTH \}) \\
			&amp; =  6/8 \\
			&amp; =  3/4 ,  \\
			P(Y=1) &amp; =  P(\{HHH,TTT\}) \\
			&amp; =  2/8 \\
			&amp; =  1/4 , \\
			P(Y=2) &amp; = P(\emptyset) \\
			&amp; = 0 .
			\end{array}</me></p></example>



			<p>Consider an arbitrary probability space <m>(S,P)</m> and let
				<m>X : S \rightarrow \mathbb{R}</m> be a random variable. Using <xref ref="def-probability-event"/>
				and <xref ref="def-X-equals-x"/>, the probability of the event <q><m>X=x</m></q>,
			i.e., the probability that <m>X</m> is equal to <m>x</m>, is equal to
			<me>\begin{array}{rl}
			P(X=x) &amp; =  P( \{ \omega \in S : X(\omega) = x \} ) \\
			&amp; =  \sum_{\omega: X(\omega) = x} P(\omega) .
			\end{array}</me></p>

			<p>We have interpreted <q><m>X=x</m></q> as being an event. We extend this to more
			general statements involving <m>X</m>. For example, <q><m>X \geq x</m></q> denotes
			the event
			<me>\{ \omega \in S : X(\omega) \geq x \} .
			</me>
			For our three-coin example, the random variable <m>X</m> can take each of
			the values <m>0</m>, <m>1</m>, <m>2</m>, and <m>3</m> with a positive probability.
			As a result, <q><m>X \geq 2</m></q> denotes the event <q><m>X=2</m> or <m>X=3</m></q>,
			and we have
		<me>	\begin{array}{rl}
			P(X \geq 2) &amp; = P(X=2 \vee X=3) \\
			&amp; =  P(X=2) + P(X=3) \\
			&amp; = 3/8 + 1/8 \\
			&amp; =  1/2 .
			\end{array}</me></p>



	<p>In <xref ref="subsec3_2_2-independent-events"/>, we have defined the notion of two events
	being independent. The following definition extends this to random
	variables.
	</p>
	<definition  xml:id="def-independent-random-variables">
		<title>Independent Random Variables</title><idx>Independent Random Variables</idx>
		<statement><p>
	Let <m>(S,P)</m> be a probability space and let <m>X</m> and <m>Y</m> be two random
	variables on <m>S</m>. We say that <m>X</m> and <m>Y</m> are <em>independent</em> if for
	all real numbers <m>x</m> and <m>y</m>, the events <q><m>X=x</m></q> and <q><m>Y=y</m></q> are
	independent, i.e.,
	<me> P(X=x \wedge Y=y) = P(X=x) \cdot P(Y=y) . </me>
</p></statement></definition>

	<p>There are two ways to generalize the notion of two random variables being independent
	to sequences of random variables: </p>

<definition  xml:id="def-pairwise-mutually-independent-rvs">
	<title>Pairwise and Mutually Independent Random Variables</title><idx>Pairwise and Mutually Independent Random Variables</idx>
	<statement><p>

	Let <m>(S,P)</m> be a probability space, let <m>n\geq 2</m>, and let
	<m>X_1,X_2,\ldots,X_n</m> be a sequence of random variables on <m>S</m>.
	<ol>
	<li> <p>We say that this sequence is <em>pairwise independent</em> if
	for all real numbers <m>x_1,x_2,\ldots,x_n</m>, the sequence
	<q><m>X_1=x_1</m></q>, <q><m>X_2=x_2</m></q>, <m>\ldots</m>, <q><m>X_n=x_n</m></q> of events is
	pairwise independent.</p></li>
	<li><p>We say that this sequence is <em>mutually independent</em> if
	for all real numbers <m>x_1,x_2,\ldots,x_n</m>, the sequence
	<q><m>X_1=x_1</m></q>, <q><m>X_2=x_2</m></q>, <m>\ldots</m>, <q><m>X_n=x_n</m></q> of events is
	mutually independent.</p></li>
</ol></p></statement></definition>

		</subsection>
		<subsection xml:id="subsec3_2_4-bayes-theorem"><title>Bayes' Theorem</title><idx>Bayes' Theorem</idx>
			<introduction>
				<!-- From Wikipedia		and http://wiki.stat.ucla.edu/socr/index.php/AP_Statistics_Curriculum		 -->
				<p>
					<blockquote>
						<p>
							Bayes' theorem is to the theory of probability what the Pythagorean theorem is to geometry.</p>
						<attribution>
							Sir Harold Jeffreys
							</attribution></blockquote>

					Bayes' theorem (alternatively Bayes' law or Bayes' rule) describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes' theorem, a person's age can be used to more accurately assess the probability that they have cancer, compared to the assessment of the probability of cancer made without knowledge of the person's age.</p>

				<p>One of the many applications of Bayes' theorem is Bayesian inference, a particular approach to statistical inference.</p></introduction>

			<definition  xml:id="def-bayes-rule">
				<title>Bayes' Theorem</title><idx>Bayes' Theorem</idx>
				<statement><p>
			Let <m>A</m> and <m>B</m> be events in some probability space <m>(S,P)</m>, such
			that <m>P(A) \neq 0</m> and <m>P(B) \neq 0</m>, then:
			<me> \begin{array}{rl}
				P(A \mid B)  &amp;= \frac{P(B \mid A) \cdot P(A)}{P(B)} \\
				&amp;= \frac{P(B \mid A) \cdot P(A)}{(P(B \mid A) \cdot P(A)) + (P(B \mid \overline{A}) \cdot P(\overline{A}))}\end{array} .
			</me></p></statement></definition>

			<example xml:id="ex-bayes-balls-boxes"><title>Probability of having chosen from a particular urn</title>
				<p>We have two urns <m>U_i</m> and <m>U_j</m>. <m>U_i</m> contains 2 black balls and 3 white balls. <m>U_j</m> contains 1 black ball and 1 white ball. An urn is chosen at random, then a ball is chosen at random from it. If a black ball is chosen, what is the probability it came from urn <m>U_i</m>?</p>

				<p>Let <m>I</m> be the event urn <m>U_i</m> is chosen and <m>B</m> be the event a black ball is chosen.
					<ol>
						<li><p>Note that the probability of choosing either urn is <m>P(I) = P(\overline{I}) = 1/2</m> and the probability that we chose a black ball from urn <m>U_i</m> is  <m>P(B \mid I) = 2/5</m>.</p></li>
							<li><p>Next we need to know the probability of choosing a black ball no matter which urn was chosen. This is the probability of choosing a black ball from either urn.
									<me>\begin{array}{rl}
										P(B) &amp; = P(B|I)P(I) + P(B|\overline{I})P(\overline{I})\\
										    &amp; = (2/5)(1/2) + (1/2)(1/2)\\
												&amp; = (1/5) + (1/4)\\
												&amp; = (9/20)
												\end{array}
								</me></p></li>
							<li><p>Calculate the probability urn <m>U_i</m> is where the black ball came from <m>P(I|B)</m> using Bayes' Rule:
								<me>	\begin{array}{rl}
									P(I|B) &amp; = \frac{P(B|I)P(I)}{P(B)}\\
									&amp; =\frac{(2/5)(1/2)}{9/20}\\
									&amp; = \frac{1/5}{(9/20)}\\
									&amp; = \frac{4}{9}
									\end{array}</me></p></li>
						</ol>
						</p></example>
		<p>Bayes' Theorem can be expanded to calculate the conditional probability from more than two events, <m>A, \overline{A}</m>.</p>
		<definition  xml:id="def-generalized-bayes-rule">
			<title>Generalized Bayes' Theorem</title><idx>Bayes' Theorem (Generalized)</idx>
			<statement><p>
					Let <m>B</m> be an event and <m>A_1, A_2, \dots, A_n</m> be mutually exclusive events in some probability space <m>(S,P)</m>, such
					that <m>P(A_i) \neq 0</m> and <m>P(B) \neq 0</m> then:
					<me> \begin{array}{rl}
						P(A_j \mid B)  &amp;= \frac{P(B \mid A_j) \cdot P(A_j)}{\sum_{i=1}^{n}(P(B \mid A_i) \cdot P(A_i))}
						\end{array}.
						</me></p></statement></definition>

			</subsection>

			<exercises><title>Exercises for Section 3.2</title>

			<exercise number="1">
			  <statement>
			    <p>
						Assume that <m>E</m> and <m>F</m> are two events with positive
						probabilities.  Show that if <m>P(E|F) = P(E)</m>, then <m>P(F|E) = P(F)</m>.
			    </p>
			  </statement>
			  <solution>
			    <p>
			      If <m>P(E|F) = P(E)</m> then the two events are independent.
						<me>	\begin{array}{rl}
							P(E) &amp;  = \frac{P(E \cap F)}{P(F)}\\
							P(E) * P(F) &amp; = P(E \cap F) \\
							P(F) &amp; = \frac{P(E \cap F)}{P(E)} \\
							P(F) &amp; = P(F|E).
							\end{array}</me>
			    </p>
			  </solution>
			</exercise>

			<exercise number="2">
			  <statement>
			    <p>
						A coin is tossed three times.  What is the probability that exactly two heads occur, given that
					 <ol>
					   <li>
					     <p>
					       the first outcome was a head?
					     </p>
					   </li>
						 <li>
						   <p>
						     the first outcome was a tail?
						   </p>
						 </li>
						 <li>
						   <p>
						     the first two outcomes were heads?
						   </p>
						 </li>
						 <li>
						   <p>
						     the first two outcomes were tails?
						   </p>
						 </li>
						 <li>
						   <p>
						     the first outcome was a head and the third outcome was a head?
						   </p>
						 </li>
					 </ol>
			    </p>
			  </statement>
			</exercise>

			<!-- Grinstead 4.1 ex 1 - 9 -->
			<exercise number="3">
			  <statement>
			    <p>
						A die is rolled twice.  What is the probability that the sum of the faces	is greater than 7, given that
				<ol marker="(a)">
				  <li>
				    <p>
				      the first outcome was a 4?
				    </p>
				  </li>
					<li>
					  <p>
					    the first outcome was greater than 3?
					  </p>
					</li>
					<li>
					  <p>
					    the first outcome was a 1?
					  </p>
					</li>
					<li>
					  <p>
					    the first outcome was less than 5?
					  </p>
					</li>
				</ol>
			    </p>
			  </statement>
			  <solution>
			    <p>				<ol marker="(a)">
									  <li>
									    <p>
									      <m>\frac{1}{2} </m>
									    </p>
									  </li>
										<li>
										  <p>
										    <m>\frac{2}{3} </m>
										  </p>
										</li>
										<li>
										  <p>
										    <m>0</m>
										  </p>
										</li>
										<li>
										  <p>
										    <m>\frac{1}{4} </m>
										  </p>
										</li>
									</ol>

			    </p>
			  </solution>
			</exercise>

			<exercise number="4">
			  <statement>
			    <p>
						A card is drawn at random from a deck of cards.  What is the probability
						that
						<ol marker="(a)">
						  <li>
						    <p>
						      it is a heart, given that it is red?
						    </p>
						  </li>
							<li>
							  <p>
									it is higher than a 10, given that it is a heart? (Interpret J, Q, K, A as 11, 12, 13, 14.)
							  </p>
							</li>
							<li>
							  <p>
							    it is a jack, given that it is red?
							  </p>
							</li>
						</ol>
			    </p>
			  </statement>
			</exercise>

			<exercise number="5">
			  <statement>
			    <p>
						A coin is tossed three times.  Consider the following events:
						<dl>
						  <li>
						    <title>A</title>
						    <p>
						      Heads on the first toss.
						    </p>
						  </li>
							<li>
								 <title>B</title>
							  <p>
							    Tails on the second.
							  </p>
							</li>
							<li>
								 <title>C</title>
								<p>
									Heads on the third toss.
								</p>
							</li>
							<li>
								 <title>D</title>
								<p>
									All three outcomes the same (HHH or TTT).
								</p>
							</li>
							<li>
								 <title>E</title>
								<p>
									Exactly one head turns up.
								</p>
							</li>
						</dl>
			    </p>
					<p>
						<ol marker="(a)">
						  <li>
						    <p>
						      Which of the following pairs of these events are independent?
									<ol>
									  <li>
									    <p>
									      <m>A,B</m>
									    </p>
									  </li>
										<li>
										  <p>
										    <m>A,D</m>
										  </p>
										</li>
										<li>
										  <p>
										    <m>A,E</m>
										  </p>
										</li>
										<li>
										  <p>
										    <m>D,E</m>
										  </p>
										</li>
									</ol>
						    </p>
						  </li>
							<li>
							  <p>
									Which of the following triples of these events are
									independent?
									<ol>
										<li>
											<p>
												<m>A, B, C</m>
											</p>
										</li>
										<li>
											<p>
												<m>A, B, D</m>
											</p>
										</li>
										<li>
											<p>
												<m>C, D, E</m>
											</p>
										</li>
									</ol>
							  </p>
							</li>
						</ol>
				</p>
			  </statement>
			  <solution>
			    <p>
						<ol marker="(a)">
						  <li>
						    <p>
						      (1) and (2)
						    </p>
						  </li>
							<li>
							  <p>
							    (1)
							  </p>
							</li>
						</ol>
			    </p>
			  </solution>
			</exercise>

			<exercise number="6">
			  <statement>
			    <p>
						From a deck of five cards numbered 2, 4, 6, 8, and 10, respectively, a card is drawn at random and replaced.  This is done three times.  What is the probability that the card numbered 2 was drawn exactly two times, given that	the sum of the numbers on the three draws is 12?
			    </p>
			  </statement>
			</exercise>

	<exercise number="7">
	  <statement>
	    <p>
				A coin is tossed twice.  Consider the
			 following events.
			 <dl>
			   <li>
			     <title>A</title>
			     <p>
			       Heads on the first toss.
			     </p>
			   </li>
				 <li>
					 <title>B</title>
					 <p>
						 Heads on the second toss.
					 </p>
				 </li>
				 <li>
					 <title>C</title>
					 <p>
						 The two tosses come out the same.
					 </p>
				 </li>
			 </dl>

			 <ol>
			   <li>
			     <p>
						 Show that <m>A</m>,~<m>B</m>,~<m>C</m> are pairwise independent but not independent.
			     </p>
			   </li>
				 <li>
				   <p>
				     Show that <m>C</m> is independent of <m>A</m> and <m>B</m> but not of <m>A \cap B</m>.
				   </p>
				 </li>
			 </ol>
	    </p>
	  </statement>
	  <solution>
	    <p>
	     <ol>
	       <li>
	         <p>
	           We have
						 <me>	\begin{array}{rl}
							 P(A \cap B) = P(A \cap C) = P(B \cap C) &amp;  = \frac{1}{4}\\
							 P(A)P(B) = P(A)P(C) = P(B)P(C) &amp;  = \frac{1}{4}\\
							 P(A \cap B \cap C) = \frac{1}{4} &amp; \neq P(A)P(B)P(C)  = \frac{1}{8}.
							 \end{array}</me>
	         </p>
	       </li>
				 <li>
				   <p>
						 A and C; and C and B are independent:
						 <me>	\begin{array}{rl}
							 P(A \cap C) = P(A)P(C) &amp;  = \frac{1}{4}\\
							 P(C \cap B) = P(B)P(C) &amp;  = \frac{1}{4}\\
							 P(C \cap (A \cap B)) = \frac{1}{4} &amp; \neq P(C)P(A \cap B)  = \frac{1}{8}.
							 \end{array}</me>
				   </p>
				 </li>
	     </ol>
	    </p>
	  </solution>
	</exercise>

	<exercise number="8">
	  <statement>
	    <p>
				Let <m>S = \{a,b,c,d,e,f\}</m>.  Assume that <m>P(a) = P(b) = 1/8</m> and
				<m>P(c) = P(d) = P(e) = P(f) = 3/16</m>.  Let <m>A</m>,~<m>B</m>, and~<m>C</m> be the events <m>A = \{d,e,a\}</m>,
				<m>B = \{c,e,a\}</m>, <m>C = \{c,d,a\}</m>.  Show that <m>P(A \cap B \cap C) = P(A)P(B)P(C)</m> but no two of these
				events are independent.
	    </p>
	  </statement>
	</exercise>

	<exercise number="9">
	  <statement>
	    <p>
			 <!-- From Grinstead Example 4.5 p.135-6 -->
				  We have two urns <m>U_a</m> and <m>U_b</m>. <m>U_a</m> contains 9 black balls and 6 white balls. <m>U_b</m> contains 3 black balls and 1 white ball. An urn is chosen at random, then a ball is chosen at random from it. Let <m>A</m> be the event urn <m>U_a</m> is chosen and <m>W</m> be the event a white ball is chosen.
	    </p>
			<p>
			  Calculate <m>P(A|W)</m> using Bayes' Rule
			</p>
	  </statement>
	  <solution>
	    <p>
				<me>
				\begin{array}{rl}
			    P(A|W) &amp; = \frac{P(W|A)P(A)}{P(W|A)P(A) + P(W|B)P(B)}\\
			    &amp; =\frac{(6/15)(1/2)}{(6/15)(1/2) + (1/4)(1/2)}\\
			   &amp; = \frac{6/30}{(6/30) + (1/8)}\\
			   &amp; = \frac{2/10}{(78/240)}\\
			   &amp; = \frac{48}{78}\\
				 &amp; = \frac{8}{13}
			 \end{array}
			 </me>
	    </p>
	  </solution>
	</exercise>

	<exercise number="10">
	  <statement>
	    <p>Suppose that 1% of the patients tested in a hospital
are infected with a virus. Furthermore, suppose that when a test for the virus is given, 98% of the patients <em>actually infected</em> with the virus test positive, and that 1% of the patients <em>not infected</em> still test positive
for it. What is the probability that:

<ol marker="(a)">
  <li>
    <p>
      a patient testing positive is actually infected with the virus?
    </p>
  </li>
	<li>
	  <p>
	    a patient testing positive is <em>not</em> infected with the virus?
	  </p>
	</li>
	<li>
	  <p>
	    a patient testing negative is infected with the virus?
	  </p>
	</li>
	<li>
	  <p>
			a patient testing negative is <em>not</em> infected with the virus?
	  </p>
	</li>
</ol>
	    </p>
	  </statement>

	</exercise>
		</exercises>
	</section>
