<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-grammars">
  <title>Grammars</title>
  <idx>
    <h>Grammars</h>
  </idx>
  <introduction>
    <p>
      Both natural languages, such as English and the
      artificial languages used for programming have a structure
      known as grammar or syntax. In order to form legal sentences
      or programs, the parts of the language must be fit together
      according to certain rules. For natural languages, the
      rules are somewhat informal (although high-school English
      teachers might have us believe differently). For programming
      languages, the rules are absolute, and programs that violate
      the rules will be rejected by a compiler.
    </p>

    <p> In this section, we will study formal grammars and languages defined by them. The languages
      we look at will, for the most part, be <q>toy</q> languages, compared to natural languages or
      even to programming languages, but the ideas and techniques are basic to any study of
      language. In fact, many of the ideas arose almost simultaneously in the 1950s in the work of
      linguists who were studying natural language and programmers who were looking for ways to
      specify the syntax of programming languages. </p>

    <p> The grammars in this section are <term>generative grammars</term><idx>
        <h>Generative Grammars</h>
      </idx>. A generative grammar is a set of rules that can be
      used to generate all the legal strings in a language. We will also consider the closely
      related idea of <term>parsing</term>. To parse a string means to determine how that string can
      be generated according to the rules. </p>
    <p>
      This section is a continuation of the preceding section.
      Like a regular expression, a grammar is a way to specify a possibly
      infinite language with a finite amount of information. In fact,
      we will see that every regular language can be specified
      by a certain simple type of grammar. We will also see that some languages
      that can be specified by grammars are not regular.
    </p>
  </introduction>
  <subsection xml:id="subsec-context-free-grammars">
    <title>Context-free Grammars</title>
    <idx>
      <h>Context-free Grammar</h>
    </idx>
    <p> In its most general form, a grammar is a set of <term>rewriting rules</term> <idx>
        <h>Rewriting
          Rule</h>
      </idx>. A rewriting rule specifies that a certain string of symbols can be
      substituted for all or part of another string. If <m>w</m> and <m>u</m> are strings, then <m>w\PRODUCES
      u</m> is a rewriting rule that specifies that the string <m>w</m> can be replaced by the
      string <m>u</m>. The symbol <q>
        <m>\PRODUCES</m>
      </q> is read <q>can be rewritten as.</q> Rewriting rules are also
      called <term>production rules</term> <idx>Production Rule</idx> or <term>
        productions</term>, and <q>
        <m>\PRODUCES</m>
      </q> <notation>
        <usage><m>\PRODUCES</m></usage>
        <description>Produces, as in <m>A\PRODUCES ab</m></description>
      </notation> can also be
      read as <q>produces.</q> For example, if we consider strings over the alphabet <m>\{a,b,c\}</m>,
      then the production rule <m>aba\PRODUCES cc</m> can be applied to the string <m>abbabac</m> to
      give the string <m>abbccc</m>. The substring <m>aba</m> in the string <m>abbabac</m> has been
      replaced with <m>cc</m>. </p>

    <p> In a <term>context-free grammar</term>, every rewriting rule has the form <m>A\PRODUCES w</m>,
      where <m>A</m> is single symbol and <m>w</m> is a string of zero or more symbols. The symbols
      that occur on the left-hand sides of production rules in a context-free grammar are called <term>non-terminal
      symbols</term><idx>
        <h>non-terminal symbol</h>
      </idx>. <aside>
        <p> The grammar is <q>context-free</q> in the sense that <m>w</m> can be substituted for <m>
      A</m> wherever <m>A</m> occurs in a string, regardless of the surrounding context in which <m>
      A</m> occurs. </p>
      </aside>By convention, the non-terminal symbols are usually uppercase
      letters. The strings on the right-hand sides of the production rules can include non-terminal
      symbols as well as other symbols, which are called <idx>terminal symbol</idx> <term>terminal
      symbols</term>. By convention, the terminal symbols are usually lowercase letters. Here are
      some typical production rules that might occur in context-free grammars: <md>
        <mrow> A\amp \PRODUCES aAbB\</mrow>
        <mrow>S \amp \PRODUCES SS</mrow>
        <mrow> C\amp \PRODUCES Acc</mrow>
        <mrow> B\amp \PRODUCES b</mrow>
        <mrow>A \amp \PRODUCES\EMPTYSTRING</mrow>
      </md> In the last rule in this list, <m>
      \EMPTYSTRING</m> represents the empty string, as usual. For example, this rule could be
      applied to the string <m>aBaAcA</m> to produce the string <m>aBacA</m>. The first occurrence
      of the symbol <m>A</m> in <m>aBaAcA</m> has been replaced by the empty string---which is just
      another way of saying that the symbol has been dropped from the string. </p>

    <p> In every context-free grammar, one of the non-terminal symbols is designated as the <term>start
      symbol</term> of the grammar. The start symbol is often, though not always, denoted by <m>S</m>.
      When the grammar is used to generate strings in a language, the idea is to start with a string
      consisting of nothing but the start symbol. Then a sequence of production rules is applied.
      Each application of a production rule to the string transforms the string to a new string. If
      and when this process produces a string that consists purely of terminal symbols, the process
      ends. That string of terminal symbols is one of the strings in the language generated by the
      grammar. In fact, the language consists precisely of all strings of terminal symbols that can
      be produced in this way. </p>

    <p> As a simple example, consider a grammar that has three production rules: <md>
        <mrow>S\PRODUCES aS</mrow>
        <mrow> S\PRODUCES bS </mrow>
        <mrow> S\PRODUCES b </mrow>
      </md> In this example, <m>S</m> is the only non-terminal
      symbol, and the terminal symbols are <m>a</m> and <m>b</m>. Starting from the string <m>S</m>,
      we can apply any of the three rules of the grammar to produce either <m>aS</m> , <m>bS</m>, or <m>
      b</m>. Since the string <m>b</m> contains no non-terminals, we see that <m>
        b</m> is one of the strings in the language generated by this grammar. The strings <m>aS</m>
      and <m>bS</m> are not in that language, since they contain the non-terminal symbol <m>S</m>,
      but we can continue to apply production rules to these strings. From <m>aS</m>, for example,
      we can obtain <m>aaS</m>, <m>abS</m>, or <m>ab</m>. From <m>abS</m>, we go on to obtain <m>
      abaS</m>, <m>abbS</m>, or <m>abb</m>. The strings <m>ab</m> and <m>abb</m> are in the language
      generated by the grammar. It's not hard to see that any string of <m>a\text{'s}</m> and <m>b\text{'s}</m>
      that ends with a <m>b</m> can be generated by this grammar, and that these are the only
      strings that can be generated. That is, the language generated by this grammar is the regular
      language specified by the regular expression <m>(a\REOR b)^{*}b</m>. </p>
    <p>
      It's time to give some formal definitions of the concepts which
      we have been discussing.
    </p>
    <definition xml:id="def-context-free-grammar">
      <title>Context-free Grammar</title>
      <statement>
        <p> A <term>context-free grammar</term> is a 4-tuple <m>(V,\Sigma,P,S)</m>, where: <ol>
            <li>
              <p>
                <m>V</m> is a finite set of symbols. The elements of <m>V</m> are the non-terminal
          symbols of the grammar. </p>
            </li>
            <li>
              <p>
                <m>\Sigma</m> is a finite set of symbols such that <m>V\cap\Sigma=\emptyset</m>. The
          elements of <m>\Sigma</m> are the terminal symbols of the grammar. </p>
            </li>
            <li>
              <p>
                <m>P</m> is a set of production rules. Each rule is of the form <m>A\PRODUCES w</m>
          where <m>A</m> is one of the symbols in <m>V</m> and <m>w</m> is a string in the language <m>
          (V\cup\Sigma)^*</m>. </p>
            </li>
            <li>
              <p>
                <m>S\in V</m>. <m>S</m> is the start symbol of the grammar. </p>
            </li>
          </ol>
        </p>
      </statement>
    </definition>
    <p> Even though this is the formal definition, grammars are often specified informally simply by
      listing the set of production rules. When this is done it is assumed, unless otherwise
      specified, that the non-terminal symbols are just the symbols that occur on the left-hand
      sides of production rules of the grammar. The terminal symbols are all the other symbols that
      occur on the right-hand sides of production rules. The start symbol is the symbol that occurs
      on the left-hand side of the first production rule in the list. Thus, the list of production
      rules <md>
        <mrow> T\amp\PRODUCES TT </mrow>
        <mrow>T \amp \PRODUCES A</mrow>
        <mrow>A \amp \PRODUCES aAa </mrow>
        <mrow>A \amp \PRODUCES bB </mrow>
        <mrow>B \amp \PRODUCES bB </mrow>
        <mrow>B \amp \PRODUCES \EMPTYSTRING </mrow>
      </md> specifies a grammar <m>
      G=(V,\Sigma,P,T)</m> where <m>V</m> is <m>\{T,A,B\}</m>, <m>\Sigma</m> is <m>\{a,b\}</m>, and <m>
      T</m> is the start symbol. <m>P</m>, of course, is a set containing the six production rules
      in the list. </p>


    <p> Let <m>G=(V,\Sigma,P,S)</m> be a context-free grammar. Suppose that <m>x</m> and <m>y</m>
      are strings in the language <m>(V\cup\Sigma)^*</m>. The notation <m>x\YIELDS_G y</m> <notation>
        <usage><m>\YIELDS</m></usage>
        <description>Yields, as in <m>x\YIELDS_G y.</m>String <m>y</m> can be obtained from a string <m>
      x</m> by applying one production rule in <m>G</m></description>
      </notation>is used to
      express the fact that <m>y</m> can be obtained from <m>x</m> by applying one of the production
      rules in <m>P</m>. To be more exact, we say that <m>x\YIELDS_G y</m> if and only if there is a
      production rule <m>A\PRODUCES w</m> in the grammar and two strings <m>u</m> and <m>v</m> in
      the language <m>(V\cup\Sigma)^*</m> such that <m>x=uAv</m> and <m>y=uwv</m>. The fact that <m>
      x=uAv</m> is just a way of saying that <m>A</m> occurs somewhere in <m>x</m>. When the
      production rule <m>A\PRODUCES w</m> is applied to substitute <m>w</m> for <m>A</m> in <m>uAv</m>,
      the result is <m>uwv</m>, which is <m>y</m>. Note that either <m>u</m> or <m>v</m> or both can
      be the empty string. </p>

    <p> If a string <m>y</m> can be obtained from a string <m>x</m> by applying a sequence of zero
      or more production rules, we write <m>x\YIELDS_G^* y</m> <notation>
        <usage><m>\YIELDS_G^*</m></usage>
        <description>Yields in zero or more steps.</description>
      </notation>. In most cases,
      the <q>
        <m>G</m>
      </q> in the notations <m>\YIELDS_G</m> and <m>\YIELDS_G^*</m> will be omitted,
      assuming that the grammar in question is understood. Note that <m>\YIELDS</m> is a relation on
      the set <m>(V\cup\Sigma)^*</m>. The relation <m>\YIELDSTAR</m> is the reflexive, transitive
      closure of that relation. (This explains the use of <q>
        <m>*</m>
      </q>, which is usually used to denote the transitive, but not necessarily
      reflexive, closure of a relation. In this case, <m>
        \YIELDSTAR</m> is reflexive as well as transitive since <m>x\;\YIELDSTAR x</m> is true for
      any string <m>x</m>.) For example, using the grammar that is defined by the above list of
      production rules, we have <md>
        <mrow> aTB\amp \YIELDS aTTB</mrow>
        <mrow> \amp \YIELDS aTAB </mrow>
        <mrow> \amp \YIELDS aTAbB </mrow>
        <mrow> \amp \YIELDS aTbBbB</mrow>
        <mrow> \amp \YIELDS aTbbB</mrow>
      </md> From this, it follows that <m>aTB\;\YIELDSTAR
      aTbbB</m>. The relation <m>\YIELDS</m> is read <q>yields</q> or <q>produces</q> while <m>
      \YIELDSTAR</m> can be read <q>yields in zero or more steps</q> or <q>produces in zero or more
      steps.</q> The following theorem states some simple facts about the relations <m>\YIELDS</m>
      and <m>
        \YIELDSTAR</m>:</p>

    <theorem xml:id="thm-context-free-grammar">
      <statement>
        <p> Let <m>G</m> be the context-free grammar <m>(V,\Sigma,P,S)</m>. Then: <ol>
            <li>
              <p> If <m>x</m> and <m>y</m> are strings in <m>(V\cup\Sigma)^*</m> such that <m>x\YIELDS
          y</m>, then <m>x\;\YIELDSTAR y</m>. </p>
            </li>
            <li>
              <p> If <m>x</m>, <m>y</m>, and <m>z</m> are strings in <m>(V\cup\Sigma)^*</m> such
          that <m>x\;\YIELDSTAR y</m> and <m>y\;\YIELDSTAR z</m>, then <m>x\;\YIELDSTAR z</m>. </p>
            </li>
            <li>
              <p> If <m>x</m> and <m>y</m> are strings in <m>(V\cup\Sigma)^*</m> such that <m>x\YIELDS
          y</m>, and if <m>s</m> and <m>t</m> are any strings in <m>(V\cup\Sigma)^*</m>, then <m>sxt\YIELDS
          syt</m>. </p>
            </li>
            <li>
              <p> If <m>x</m> and <m>y</m> are strings in <m>(V\cup\Sigma)^*</m> such that <m>x\;\YIELDSTAR
          y</m>, and if <m>s</m> and <m>t</m> are any strings in <m>(V\cup\Sigma)^*</m>, then <m>sxt\;\YIELDSTAR
          syt</m>. </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
        <p> Parts 1 and 2 follow from the fact that <m>\YIELDSTAR</m> is the transitive closure of <m>
          \YIELDS</m>. Part 4 follows easily from Part 3. (I leave this as an exercise.) To prove
          Part 3, suppose that <m>x</m>, <m>y</m>, <m>s</m>, and <m>t</m> are strings such that <m>x\YIELDS
          y</m>. By definition, this means that there exist strings <m>u</m> and <m>v</m> and a
          production rule <m>A\PRODUCES w</m> such that <m>x=uAv</m> and <m>y=uwv</m>. But then we
          also have <m>sxt=suAvt</m> and <m>syt=suwvt</m>. These two equations, along with the
          existence of the production rule <m>A\PRODUCES w</m> show, by definition, that <m>sxt\YIELDS
          syt</m>.</p>
      </proof>
    </theorem>

    <p> We can use <m>\YIELDSTAR</m> to give a formal definition of the language generated by a
      context-free grammar: </p>

    <definition xml:id="def-context-free-language">
      <title>Context-free Language</title>
      <statement>
        <p> Suppose that <m>G=(V,\Sigma,P,S)</m> is a context-free grammar. Then the language
          generated by <m>G</m> is the language <m>L(G)</m> over the alphabet <m>\Sigma</m> defined
          by 
          <me>L(G)=\{w\in \Sigma^* | S\YIELDS_G^* w\}</me> 
          That is, <m>L(G)</m> contains any
          string of terminal symbols that can be obtained by starting with the string consisting of
          the start symbol, <m>S</m>, and applying a sequence of production rules. </p>
        <p> A language <m>L</m> is said to be a <term>context-free language</term> if there is a
          context-free grammar <m>G</m> such that <m>L(G)</m> is <m>L</m>. Note that there might be
          many different context-free grammars that generate the same context-free language. Two
          context-free grammars that generate the same language are said to be <idx>equivalent
          grammars</idx> <term>equivalent</term>. </p>
      </statement>
    </definition>


    <p> Suppose <m>G</m> is a context-free grammar with start symbol <m>S</m> and suppose <m>w\in
      L(G)</m>. By definition, this means that there is a sequence of one or more applications of
      production rules which produces the string <m>w</m> from <m>S</m>. This sequence has the form <m>S\YIELDS
      x_1\YIELDS x_2\YIELDS\cdots\YIELDS w</m>. Such a sequence is called a <term>derivation</term> of <m>w</m>
      (in the grammar <m>G</m>). Note that <m>w</m> might have more than one derivation. That is, it
      might be possible to produce <m>w</m> in several different ways. </p>

    <p> Consider the language <m>L=\{a^nb^n| n\in\N\}</m>. We already know that <m>L</m> is not a
      regular language. However, it is a context-free language. That is, there is a context-free
      grammar such that <m>L</m> is the language generated by <m>G</m>. This gives us our first
      theorem about grammars: </p>
      
      <theorem xml:id="thm-non-regular-context-free">
        <statement>
          <p> Let <m>L</m> be the language <m>L=\{a^nb^n| n\in\N\}</m>. Let <m>G</m> be the
      context-free grammar <m>(V,\Sigma,P,S)</m> where <m>V=\{S\}</m>, <m>\Sigma=\{a,b\}</m> and <m>
      P</m> consists of the productions <md>
              <mrow>S \amp \PRODUCES aSb </mrow>
              <mrow>S \amp \PRODUCES \EMPTYSTRING </mrow>
            </md> Then <m>L=L(G)</m>, so
      that <m>L</m> is a context-free language. In particular, there exist context-free languages
      which are not regular.</p>
        </statement>
        <proof>
          <p> To show that <m>L=L(G)</m>, we must show both that <m>L\SUB L(G)</m> and that <m>L(G)\SUB
      L</m>. To show that <m>L\SUB L(G)</m>, let <m>w</m> be an arbitrary element of <m>L</m>. By
      definition of <m>L</m>, <m>w=a^nb^n</m> for some <m>n\in\N</m>. We show that <m>w\in L(G)</m>
      by induction on <m>n</m>. In the case where <m>n=0</m>, we have <m>w=\EMPTYSTRING</m>. Now, <m>\EMPTYSTRING\in
      L(G)</m> since <m>\EMPTYSTRING</m> can be produced from the start symbol <m>S</m> by an
      application of the rule <m>S\PRODUCES\EMPTYSTRING</m>, so our claim is true for <m>n=0</m>.
      Now, suppose that <m>k\in\N</m> and that we already know that <m>a^kb^k\in L(G)</m>. We must
      show that <m>a^{k+1}b^{k+1}\in L(G)</m>. Since <m>S\;\YIELDSTAR a^kb^k</m>, we also have, by
      <xref ref="thm-context-free-grammar"/>, that <m>aSb\;\YIELDSTAR aa^kb^kb</m>. That is, <m>aSb\;\YIELDSTAR
      a^{k+1}b^{k+1}</m>. Combining this with the production rule <m>S\PRODUCES aSb</m>, we see that <m>S\;\YIELDSTAR
      a^{k+1}b^{k+1}</m>. This means that <m>a^{k+1}b^{k+1}\in L(G)</m>, as we wanted to show. This
      completes the proof that <m>L\SUB L(G)</m>. </p>

          <p> To show that <m>L(G)\SUB L</m>, suppose that <m>w\in L(G)</m>. That is, <m>S\;\YIELDSTAR
      w</m>. We must show that <m>w=a^nb^n</m> for some <m>n</m>. Since <m>S\;\YIELDSTAR w</m>,
      there is a derivation <m>S\YIELDS x_0\YIELDS x_1\YIELDS</m><m>\cdots</m><m>\YIELDS x_n</m>, where <m>w=x_n</m>.
      We first prove by induction on <m>n</m> that in any derivation <m>S\YIELDS x_0\YIELDS
      x_1\YIELDS</m><m>\cdots</m><m>\YIELDS x_n</m>, we must have either <m>x_n=a^nb^n</m> or <m>
      x_n=a^{n+1}Sb^{n+1}</m>. Consider the case <m>n=0</m>. Suppose <m>S\YIELDS x_0</m>. Then, we
      must have that <m>S\PRODUCES x_0</m> is a rule in the grammar, so <m>x_0</m> must be either <m>
      \EMPTYSTRING</m> or <m>aSb</m>. Since <m>\EMPTYSTRING=a^0b^0</m> and <m>aSb=a^{0+1}Sb^{0+1}</m>
      , <m>x_0</m> is of the required form. Next, consider the inductive case. Suppose that <m>k\gt
      1</m> and we already know that in any derivation <m>S\YIELDS x_0\YIELDS
      x_1\YIELDS\cdots\YIELDS x_k</m>, we must have <m>x_k=a^kb^k</m> or <m>x=a^{k+1}Sb^{k+1}</m>.
      Suppose that <m>S\YIELDS x_0\YIELDS x_1\YIELDS</m><m>\cdots</m><m>\YIELDS x_k\YIELDS x_{k+1}</m>. We know by
      induction that <m>x_k=a^kb^k</m> or <m>x=a^{k+1}Sb^{k+1}</m>, but since <m>x_k\YIELDS x_{k+1}</m>
      and <m>a^kb^k</m> contains no non-terminal symbols, we must have <m>x_k=a^{k+1}Sb^{k+1}</m>.
      Since <m>x_{k+1}</m> is obtained by applying one of the production rules <m>
      S\PRODUCES\EMPTYSTRING</m> or <m>S\PRODUCES aSb</m> to~<m>x_k</m>, <m>x_{k+1}</m> is either <m>a^{k+1}\EMPTYSTRING
      b^{k+1}</m> or <m>a^{k+1}aSbb^{k+1}</m>. That is, <m>x_{k+1}</m> is either <m>a^{k+1}b^{k+1}</m>
      or <m>a^{k+2}Sb^{k+2}</m>, as we wanted to show. This completes the induction. Turning back to <m>
      w</m>, we see that <m>w</m> must be of the form <m>a^nb^n</m> or of the form <m>a^nSb^n</m>.
      But since <m>w\in L(G)</m>, it can contain no non-terminal symbols, so <m>w</m> must be of the
      form <m>a^nb^n</m>, as we wanted to show. This completes the proof that <m>L(G)\SUB L</m>. </p>
        </proof>
      </theorem>

    <p> I have given a very formal and detailed proof of this theorem, to show how it can be done
      and to show how induction plays a role in many proofs about grammars. However, a more informal
      proof of the theorem would probably be acceptable and might even be more convincing. To show
      that <m>L\SUB L(G)</m>, we could just note that the derivation <m>S\YIELDS aSb\YIELDS
      a^2Sb^2\YIELDS</m><m>\cdots</m><m>\YIELDS a^nSb^n\YIELDS a^nb^n</m> demonstrates that <m>a^nb^n\in L</m>. On
      the other hand, it is clear that every derivation for this grammar must be of this form, so
      every string in <m>L(G)</m> is of the form <m>a^nb^n</m>. </p>

    <example> <!-- Example 15.5.5 -->
      <p> For another example, consider the language <m>\{a^nb^m| n\ge m\ge0\}</m>. Let's try to
        design a grammar that generates this language. This is similar to the previous example, but
        now we want to include strings that contain more <m>a\text{'s}</m> than <m>b\text{'s}</m>. The production
        rule <m>S\PRODUCES aSb</m> always produces the same number of <m>a\text{'s}</m> and <m>b\text{'s}</m>. Can
        we modify this idea to produce more <m>a\text{'s}</m> than <m>b\text{'s}</m>? </p>

      <p> One approach would be to produce a string containing just as many <m>a\text{'s}</m> as <m>b\text{'s}</m>,
        and then to add some extra <m>a\text{'s}</m>. A rule that can generate any number of <m>a\text{'s}</m> is <m>A\PRODUCES
        aA</m>. After applying the rule <m>S\PRODUCES aSb</m> for a while, we want to move to a new
        state in which we apply the rule <m>A\PRODUCES aA</m>. We can get to the new state by
        applying a rule <m>S\PRODUCES A</m> that changes the <m>S</m> into an <m>A</m>. We still
        need a way to finish the process, which means getting rid of all non-terminal symbols in the
        string. For this, we can use the rule <m>A\PRODUCES\EMPTYSTRING</m>. Putting these rules
        together, we get the grammar 
        <md>
          <mrow>S \amp \PRODUCES aSb</mrow>
          <mrow>S \amp \PRODUCES A </mrow>
          <mrow>A \amp \PRODUCES aA </mrow>
          <mrow>A \amp \PRODUCES \EMPTYSTRING </mrow>
        </md> 
        This grammar does indeed generate
        the language <m>\{a^nb^m| n\ge m\ge 0\}</m>. With slight variations on this grammar, we
        can produce other related languages. For example, if we replace the rule <m>A\PRODUCES
        \EMPTYSTRING</m> with <m>A\PRODUCES a</m>, we get the language <m>\{a^nb^m| n \gt m\ge 0\}</m>
        .
</p>
      <p>There are other ways to generate the language <m>\{a^nb^m| n\ge m\ge 0\}</m>. For
        example, the extra non-terminal symbol, <m>A</m>, is not really necessary, if we allow <m>S</m>
        to sometimes produce a single <m>a</m> without a <m>b</m>. This leads to the grammar 
        <md>
          <mrow>S \amp \PRODUCES aSb </mrow>
          <mrow>S \amp \PRODUCES aS </mrow>
          <mrow>S \amp \PRODUCES \EMPTYSTRING </mrow>
        </md>
        (But note that the rule <m>S\PRODUCES Sa</m> would not work in place of 
        <m>S\PRODUCES aS</m>, since it would allow the production
        of strings in which an <m>a</m> can follow a <m>b</m>, and there are no such strings in the
        language <m>\{a^nb^m| n\ge m\ge 0\}</m>.) And here are two more grammars that generate
        this language: 
        <md>
          <mrow> S\amp\PRODUCES AB \amp\qquad S\amp\PRODUCES ASb </mrow>
          <mrow> A\amp\PRODUCES aA \amp\qquad A\amp\PRODUCES aA </mrow>
          <mrow> B\amp\PRODUCES aBb \amp\qquad S\amp\PRODUCES\EMPTYSTRING </mrow>
          <mrow> A\amp\PRODUCES \EMPTYSTRING \amp\qquad A\amp\PRODUCES\EMPTYSTRING </mrow>
          <mrow> B\amp\PRODUCES \EMPTYSTRING \amp\qquad \amp</mrow>
        </md>
      </p>
    </example>


    <example> <!-- Example 15.5.6 -->
      <p> Consider another variation on the language <m>\{a^nb^n| n\in\N\}</m>, in which the <m>a\text{'s}</m>
        and <m>b\text{'s}</m> can occur in any order, but the number of <m>a\text{'s}</m> is still equal to the
        number of <m>b\text{'s}</m>. This language can be defined as <m>L=\{w\in\{a,b\}^*| n_a(w) = n_b(w)\}</m>. 
        This language includes strings such as <m>abbaab</m>, <m>baab</m>, and <m>
        bbbaaa</m>. </p>
      <p> Let's start with the grammar containing the rules <m>S\PRODUCES aSb</m> and <m>
        S\PRODUCES\EMPTYSTRING</m>. We can try adding the rule <m>S\PRODUCES bSa</m>. Every string
        that can be generated using these three rules is in the language <m>L</m>. However, not
        every string in <m>L</m> can be generated. A derivation that starts with <m>S\YIELDS aSb</m>
        can only produce strings that begin with <m>a</m> and end with <m>b</m>. A derivation that
        starts with <m>S\YIELDS bSa</m> can only generate strings that begin with <m>b</m> and end
        with <m>a</m>. There is no way to generate the strings <m>baab</m> or <m>abbbabaaba</m>,
        which are in the language <m>L</m>. But we shall see that any string in <m>L</m> that begins
        and ends with the same letter can be written in the form <m>xy</m> where <m>x</m> and <m>y</m>
        are shorter strings in <m>L</m>. To produce strings of this form, we need one more rule, <m>S\PRODUCES
        SS</m>. The complete set of production rules for the language <m>L</m> is <md>
          <mrow> S\amp\PRODUCES aSb </mrow>
          <mrow> S\amp\PRODUCES bSa </mrow>
          <mrow> S\amp\PRODUCES SS </mrow>
          <mrow> S\amp\PRODUCES \EMPTYSTRING </mrow>
        </md>
      </p>
      <proof>
        <p> It's easy to see that every string that can be generated using these rules is in <m>L</m>,
          since each rule introduces the same number of <m>a\text{'s}</m> as <m>b\text{'s}</m>. But we also need
          to check that every string <m>w</m> in <m>L</m> can be generated by these rules. This can
          be done by induction on the length of <m>w</m>, using the second form of the principle of
          mathematical induction. In the base case, <m>|w|=0</m> and <m>w=\EMPTYSTRING</m>. In this
          case, <m>w\in L</m> since <m>S\YIELDS\EMPTYSTRING</m> in one step. Suppose <m>|w|=k</m>,
          where <m>k\gt 0</m>, and suppose that we already know that for any <m>x\in L</m> with <m>|x|
          \lt k</m>, <m>S\;\YIELDSTAR x</m>. To finish the induction we must show, based on this
          induction hypothesis, that <m>S\;\YIELDSTAR w</m>. </p>
        <p> Suppose that the first and last characters of <m>w</m> are different. Then <m>w</m> is
          either of the form <m>axb</m> or of the form <m>bxa</m>, for some string <m>x</m>. Let's
          assume that <m>w</m> is of the form <m>axb</m>. (The case where <m>w</m> is of the form <m>
          bxa</m> is handled in a similar way.) Since <m>w</m> has the same number of <m>a\text{'s}</m> and <m>
          b\text{'s}</m> and since <m>x</m> has one fewer <m>a</m> than <m>w</m> and one fewer <m>b</m>
          than <m>w</m>, <m>x</m> must also have the same number of <m>a\text{'s}</m> as <m>b\text{'s}</m>. That
          is <m>x\in L</m>. But <m>|x|=|w|-2 \lt k</m>, so by the induction hypothesis, <m>x\in L(G)</m>.
          So we have <m>S\;\YIELDSTAR x</m>. By <xref ref="thm-context-free-grammar" />, we get then <m>aSb\;\YIELDSTAR
          axb</m>. Combining this with the fact that <m>S\YIELDS aSb</m>, we get that <m>S\;\YIELDSTAR
          axb</m>, that is, <m>S\;\YIELDSTAR w</m>. This proves that <m>w\in L(G)</m>. </p>

        <p> Finally, suppose that the first and last characters of <m>w</m> are the same. Let's say
          that <m>w</m> begins and ends with <m>a</m>. (The case where <m>w</m> begins and ends with <m>
          b</m> is handled in a similar way.) I claim that <m>w</m> can be written in the form <m>xy</m>
          where <m>x\in L(G)</m> and <m>y\in L(G)</m> and neither <m>x</m> nor <m>y</m> is the empty
          string. This will finish the induction, since we will then have by the induction
          hypothesis that <m>S\;\YIELDSTAR x</m> and <m>S\;\YIELDSTAR y</m>, and we can derive <m>xy</m>
          from <m>S</m> by first applying the rule <m>S\PRODUCES SS</m> and then using the first <m>
          S</m> on the right-hand side to derive <m>x</m> and the second to derive <m>y</m>. </p>

        <p> It only remains to figure out how to divide <m>w</m> into two strings <m>x</m> and <m>y</m>
          which are both in <m>L(G)</m>. The technique that is used is one that is more generally
          useful. Suppose that <m>w=c_1c_2\cdots c_k</m>, where each <m>c_i</m> is either <m>a</m>
          or <m>b</m>. Consider the sequence of integers <m>r_1</m>, <m>r_2</m>, <m>\dots</m>, <m>r_k</m>
          where for each <m>i = 1, 2, \dots, k</m>, <m>r_i</m> is the number of <m>a\text{'s}</m> in <m>c_1c_2\cdots
          c_i</m> minus the number of <m>b\text{'s}</m> in <m>c_1c_2\cdots c_i</m>. Since <m>c_1=a</m>, <m>
          r_1=1</m>. Since <m>w\in L</m>, <m>r_k=0</m>. And since <m>c_k=a</m>, we must have <m>r_{k-1}=
          r_k-1 = -1</m>. Furthermore the difference between <m>r_{i+1}</m> and <m>r_i</m> is either <m>
          1</m> or <m>-1</m>, for <m>i=1,2,\dots,k-1</m>. </p>

        <p> Since <m>r_1=1</m> and <m>r_{k-1}=-1</m> and the value of <m>r_i</m> goes up or down by
          1 when <m>i</m> increases by 1, <m>r_i</m> must be zero for some <m>i</m> between 1 and <m>
          k-1</m>. That is, <m>r_i</m> cannot get from 1 to <m>-1</m> unless it passes through zero.
          Let <m>i</m> be a number between 1 and <m>k-1</m> such that <m>r_i=0</m>. Let <m>x=c_1c_2\cdots
          c_i</m> and let <m>y=c_{i+1}c_{i+2}\cdots c_k</m>. Note that <m>xy=w</m>. The fact that <m>
          r_i=0</m> means that the string <m>c_1c_2\cdots c_i</m> has the same number of <m>a\text{'s}</m>
          and <m>b\text{'s}</m>, so <m>x\in L(G)</m>. It follows automatically that <m>y\in L(G)</m> also.
          Since <m>i</m> is strictly between 1 and <m>k-1</m>, neither <m>x</m> nor <m>y</m> is the
          empty string. This is all that we needed to show to finish the proof that <m>L=L(G)</m>. </p>

        <p> The basic idea of this proof is that if <m>w</m> contains the same number of <m>a\text{'s}</m>
          as <m>b\text{'s}</m>, then an <m>a</m> at the beginning of <m>w</m> must have a <q>matching</q> <m>
          b</m> somewhere in <m>w</m>. This <m>b</m> matches the <m>a</m> in the sense that the
          corresponding <m>r_i</m> is zero, and the <m>b</m> marks the end of a string <m>x</m>
          which contains the same number of <m>a\text{'s}</m> as <m>b\text{'s}</m>. For example, in the string <m>
          aababbabba</m>, the <m>a</m> at the beginning of the string is matched by the third <m>b</m>,
          since <m>aababb</m> is the shortest prefix of <m>aababbabba</m> that has an equal number
          of <m>a\text{'s}</m> and <m>b\text{'s}</m>. </p>
      </proof>
    </example>

    <example>
      <title>Balanced Parentheses</title>
      <p> Closely related to this idea of matching <m>a\text{'s}</m> and <m>b\text{'s}</m> is the idea of <term>balanced
        parentheses</term>. Consider a string made up of parentheses, such as <c>(()(()))(())</c>.
        The parentheses in this sample string are balanced because each left parenthesis has a
        matching right parenthesis, and the matching pairs are properly nested. A careful definition
        uses the sort of integer sequence introduced in the above proof. Let <m>w</m> be a string of
        parentheses. Write <m>w=c_1c_2\cdots c_n</m>, where each <m>c_i</m> is either <c>(</c> or
        <c>)</c>. Define a sequence of integers <m>r_1</m>, <m>r_2</m>, <m>\dots</m>,<m>r_n</m>, where <m>
        r_i</m> is the number of left parentheses in <m>c_1c_2\cdots c_i</m> minus the number of
        right parentheses. We say that the parentheses in <m>w</m> are balanced if <m>r_n=0</m> and <m>
        r_i\ge0</m> for all <m>i=1,2,\dots,n</m>. The fact that <m>r_n=0</m> says that <m>w</m>
        contains the same number of left parentheses as right parentheses. The fact the <m>r_i\ge0</m>
        means that the nesting of pairs of parentheses is correct: You can't have a right
        parenthesis unless it is balanced by a left parenthesis in the preceding part of the string.
        The language that consists of all balanced strings of parentheses is context-free. It is
        generated by the grammar <md>
          <mrow> S\amp\PRODUCES (\,S\,)</mrow>
          <mrow> S\amp\PRODUCES SS </mrow>
          <mrow> S\amp\PRODUCES \EMPTYSTRING </mrow>
        </md> The proof is similar to the
        preceding proof about strings of <m>a\text{'s}</m> and <m>b\text{'s}</m>. (It might seem that I've made an
        awfully big fuss about matching and balancing. The reason is that this is one of the few
        things that we can do with context-free languages that we can't do with regular languages.) </p>

    </example>
    <p>
      Before leaving this section, we should look at a few more
      general results. Since we know that most operations on regular
      languages produce languages that are also regular, we can
      ask whether a similar result holds for context-free languages.
      We will see later that the intersection of two context-free
      languages is not necessarily context-free. Also, the
      complement of a context-free language is not necessarily
      context-free. However, some other operations on context-free
      languages do produce context-free languages.
</p>
    <theorem xml:id="thm-context-free-operations">
      <statement>
        <p> Suppose that <m>L</m> and <m>M</m> are context-free languages. Then the languages <m>L\cup
          M</m>, <m>LM</m>, and <m>L^*</m> are also context-free. </p>
      </statement>
      <proof>
        <p> I will prove only the first claim of the theorem, that <m>L\cup M</m> is context-free.
          In the exercises for this section, you are asked to construct grammars for <m>LM</m> and <m>
          L^*</m> (without giving formal proofs that your answers are correct). </p>
        <p> Let <m>G=(V,\Sigma,P,S)</m> and <m>H=(W,\Gamma,Q,T)</m> be context-free grammars such
          that <m>L=L(G)</m> and <m>M=L(H)</m>. We can assume that <m>W\cap V=\emptyset</m>, since
          otherwise we could simply rename the non-terminal symbols in <m>W</m>. The idea of the
          proof is that to generate a string in <m>L\cup M</m>, we first decide whether we want a
          string in <m>L</m> or a string in <m>M</m>. Once that decision is made, to make a string
          in <m>L</m>, we use production rules from <m>G</m>, while to make a string in <m>M</m>, we
          use rules from <m>H</m>. We have to design a grammar, <m>K</m>, to represent this process. </p>
        <p> Let <m>R</m> be a symbol that is not in any of the alphabets <m>V</m>, <m>W</m>, <m>
          \Sigma</m>, or <m>\Gamma</m>. <m>R</m> will be the start symbol of <m>K</m>. The
          production rules for <m>K</m> consist of all the production rules from <m>G</m> and <m>H</m>
          together with two new rules: <md>
            <mrow> R\PRODUCES S</mrow>
            <mrow> R\PRODUCES T </mrow>
          </md> Formally, <m>K</m> is defined to be the
          grammar <me>
            (V\cup W\cup\{R\},
            P\cup Q\cup \{R\PRODUCES S, R\PRODUCES T\},
            \Sigma\cup\Gamma, R)
</me>
          Suppose that <m>w\in L</m>. That is <m>w\in L(G)</m>, so there is a derivation <m>
          S\YIELDS_G^*w</m>. Since every rule from <m>G</m> is also a rule in <m>K</m>, if follows
          that <m>S\YIELDS_K^* w</m>. Combining this with the fact that <m>R\YIELDS_K S</m>, we have
          that <m>R\YIELDS_K^* w</m>, and <m>w\in L(K)</m>. This shows that <m>L\SUB L(K)</m>. In an
          exactly similar way, we can show that <m>M\SUB L(K)</m>. Thus, <m>L\cup M\SUB L(K)</m>. </p>
        <p> It remains to show that <m>L(K)\SUB L\cup M</m>. Suppose <m>w\in L(K)</m>. Then there is
          a derivation <m>R\YIELDS_K^*w</m>. This derivation must begin with an application of one
          of the rules <m>R\PRODUCES S</m> or <m>R\PRODUCES T</m>, since these are the only rules in
          which <m>R</m> appears. If the first rule applied in the derivation is <m>R\PRODUCES S</m>,
          then the remainder of the derivation shows that <m>S\YIELDS_K^*w</m>. Starting from <m>S</m>,
          the only rules that can be applied are rules from <m>G</m>, so in fact we have <m>
          S\YIELDS_G^*w</m>. This shows that <m>w\in L</m>. Similarly, if the first rule applied in
          the derivation <m>R\YIELDS_K^*w</m> is <m>R\PRODUCES T</m>, then <m>w\in M</m>. In any
          case, <m>w\in L\cup M</m>. This proves that <m>L(K)\SUB L\cup M</m>. </p>
      </proof>
    </theorem>
    <p>
      Finally, we should clarify the relationship between context-free
      languages and regular languages. We have already seen that
      there are context-free languages which are not regular.
      On the other hand, it turns out that every regular language
      is context-free. That is, given any regular language, there
      is a context-free grammar that generates that language. This
      means that any syntax that can be expressed by a regular expression,
      by a DFA, or by an NFA could also be expressed by a context-free
      grammar. In fact, we only need a certain restricted type of
      context-free grammar to duplicate the power of regular expressions.
</p>
    <definition xml:id="def-right-regular-grammar">
    <title>Right-Regular Grammar</title>
      <statement>
        <p> A <term>right-regular grammar</term><idx>regular grammar</idx> is a context-free grammar
          in which the right-hand side of every production rule has one of the following forms: the
          empty string; a string consisting of a single non-terminal symbol; or a string consisting
          of a single terminal symbol followed by a single non-terminal symbol. </p>
          </statement>
    </definition>
        <p> Examples of the types of production rule that are allowed in a right-regular grammar are <m>
          A\PRODUCES\EMPTYSTRING</m>, <m>B\PRODUCES C</m>, and <m>D\PRODUCES aE</m>. The idea of the
          proof is that given a right-regular grammar, we can build a corresponding <m>NFA</m> and
          <em>vice-versa</em>. The states of the <m>NFA</m> correspond to the non-terminal symbols
          of the grammar. The start symbol of the grammar corresponds to the starting state of the
          NFA. A production rule of the form <m>A\PRODUCES bC</m> corresponds to a transition in the
          NFA from state <m>A</m> to state <m>C</m> while reading the symbol <m>b</m>. A production
          rule of the form <m>A\PRODUCES B</m> corresponds to an <m>\EMPTYSTRING</m>-transition from
          state <m>A</m> to state <m>B</m> in the NFA. And a production rule of the form <m>
          A\PRODUCES\EMPTYSTRING</m> exists in the grammar if and only if <m>A</m> is a final state
          in the NFA. With this correspondence, a derivation of a string <m>w</m> in the grammar
          corresponds to an execution path through the NFA as it accepts the string <m>w</m>. I
          won't give a complete proof here. You are welcome to work through the details if you want.
          But the important fact is:
          </p>
          <theorem xml:id="thm-every-regular-context-free">
            <statement>
              <p> A language <m>L</m> is regular if and only if there is a right-regular grammar <m>
          G</m> such that <m>L=L(G)</m>. In particular, every regular language is context-free. </p>
            </statement>
          </theorem>
  </subsection>

<!-- subsec 15.5.2  BNF -->
 <subsection xml:id="subsec-bnf">
    <title>Backus-Naur Form</title>
    <idx>
      <h>Backus-Naur Form</h>
    </idx>
    <p>Context-free grammars are used to describe some aspects of
the syntax of programming languages.  However, the notation
that is used for grammars in the context of programming languages
is somewhat different from the notation introduced in the
preceding section.  The notation that is used is called
<term>Backus-Naur Form</term> or BNF.  It is named after computer
scientists John Backus and Peter Naur, who developed the
notation.  Actually, several variations of BNF exist.
I will discuss one of them here.  BNF can be used to describe
the syntax of natural languages, as well as programming languages,
and some of the examples in this section will deal with the
syntax of English.
    </p>

<p>Like context-free grammars, BNF grammars make use of production rules, non-terminals,
and terminals.  The non-terminals are usually given meaningful,
multi-character names.  Here, I will follow a common practice
of enclosing non-terminals in angle brackets, so that they can
be easily distinguished.  For example, <m>\langle noun \rangle</m> and <m>\langle sentence \rangle</m>
could be non-terminals in a BNF grammar for English, while
<m>\langle program \rangle</m> and <m>\langle if{\text -}statement \rangle</m> might be used in a BNF grammar
for a programming language.  Note that a BNF non-terminal
usually represents a meaningful <term>syntactic category</term>,
that is, a certain type of building block in the syntax of
the language that is being described, such as an adverb,
a prepositional phrase, or a variable declaration statement.
The terminals of a BNF grammar are the things that actually
appear in the language that is being described.  In the case
of natural language, the terminals are individual words.</p>

<p>In BNF production rules, I will use the symbol <q><m>::=</m></q>
in place of the <q><m>\PRODUCES</m></q> that is used in context-free grammars.
BNF production rules are more powerful than the production rules in
context-free grammars.  That is, one BNF rule might be equivalent to 
several context-free grammar rules.  As for context-free grammars,
the left-hand side of a BNF production rule is a single 
non-terminal symbol.  The right hand side can include terminals
and non-terminals, and can also use the following notations,
which should remind you of notations used in regular expressions:
<ul>
  <li>
    <p> A vertical bar, <m>\BNFALT</m>, indicates a choice of
   alternatives.  For example,
   <me>
    \langle digit \rangle ::=\ 0 \BNFALT\ 1 \BNFALT\ 2
          \BNFALT\ 3 \BNFALT\ 4 \BNFALT\ 5 \BNFALT\ 6 \BNFALT\ 7
          \BNFALT\ 8 \BNFALT\ 9
   </me>
   indicates that the non-terminal <m>\langle digit \rangle</m> can be replaced
by any one of the terminal symbols <m>0, 1, \dots,9</m>.
    </p>
  </li>
  <li>
    <p>
      Items enclosed in brackets are optional.  For example,
      <me>
        \langle declaration \rangle ::= \langle type \rangle \,\langle variable \rangle 
                 [ = \langle expression \rangle ] ;
      </me>
      says that <m>\langle declaration \rangle</m> can be replaced either
by <q><m>\langle type \rangle</m> <m>\langle variable \rangle</m>;</q> 
or by <q><m>\langle type \rangle</m> <m>\langle variable \rangle</m>
= <m>\langle expression \rangle</m>;</q>.
(The symbols <q>=</q> and <q>;</q> are terminal symbols in this rule.)
    </p>
  </li>
  <li>
    <p>
      Items enclosed between <q>[</q> and <q>]<m>\dots</m></q>
 can be repeated zero or more times.  (This has the same effect
as a <q><m>*</m></q> in a regular expression.)  For example,
<me>
  \langle integer \rangle ::=\ \langle digit \rangle [ \langle digit \rangle ]\dots
</me>
says that an <m>\langle integer \rangle</m> consists of a <m>\langle digit \rangle</m> followed
optionally by any number of additional <m>\langle digit \rangle</m>'s.  That is,
the non-terminal <m>\langle integer \rangle</m> can be replaced by <m>\langle digit \rangle</m> or
by <m>\langle digit \rangle</m><m>\langle digit \rangle</m> or by <m>\langle digit \rangle</m><m>\langle digit \rangle</m><m>\langle digit \rangle</m>, and
so on.
    </p>
  </li>
  <li>
    <p>
      Parentheses can be used as usual, for grouping. 
    </p>
  </li>
</ul>
</p>

<p>All these notations can be expressed in a context-free grammar
by introducing additional production rules.  For example, the
BNF rule <q><m>\langle sign \rangle ::= + | -</m></q> is equivalent
to the two rules, <q><m>\langle sign \rangle ::= + </m></q>
and <q><m>\langle sign \rangle ::= - </m></q>.  A rule that contains an
optional item can also be replaced by two rules.  For example,
<me>
  \langle declaration \rangle ::=\ \langle type \rangle \langle variable \rangle
                 [ = \langle expression \rangle ] ;
</me>
can be replaced by the two rules
<md>
  <mrow> \langle declaration \rangle \amp ::=\ \langle type \rangle \langle variable \rangle ;</mrow>
  <mrow> \langle declaration \rangle \amp ::=\ \langle type \rangle \langle variable \rangle
                  = \langle expression \rangle  ;</mrow>
</md>
In context-free grammars, repetition can be expressed by
using a recursive rule such as <q><m>S\PRODUCES aS</m></q>, in which the
same non-terminal symbol appears both on the left-hand side and on the right-hand
side of the rule.  BNF-style notation using <q>[</q> and <q>]<m>\dots</m></q> can
be eliminated by replacing it with a new non-terminal symbol and adding
a recursive rule to allow that symbol to repeat zero or more times.
For example, the production rule
<me>
  \langle integer \rangle ::=\ \langle digit \rangle [ \langle digit \rangle ]\dots
</me>
can be replaced by three rules using a new non-terminal symbol
<m>\langle digit{\text -}list \rangle</m> to represent a string of zero or more <m>\langle digit \rangle</m>'s:
<md>
  <mrow> \langle integer \rangle \amp ::= \langle digit \rangle \langle digit{\text -}list \rangle </mrow>
  <mrow> \langle digit{\text -}list \rangle \amp ::= \langle digit \rangle \langle digit{\text -}list \rangle </mrow>
  <mrow> \langle digit{\text -}list \rangle \amp ::= \EMPTYSTRING </mrow>
</md>

</p>
<example>
<p>As an example of a complete BNF grammar, let's look at a BNF grammar for a very small
subset of English.  The start symbol for the grammar
is <m>\langle sentence \rangle</m>, and the terminal symbols are English words.
All the sentences that can be produced from this grammar
are syntactically correct English sentences, although you wouldn't
encounter many of them in conversation.  Here is the grammar:


<md>
  <mrow> \langle sentence \rangle \amp ::=\ \langle simple{\text -}sentence \rangle [ and \langle simple{\text -}sentence \rangle ]\dots </mrow>
  <mrow> \langle simple{\text -}sentence \rangle  \amp ::=\ \langle noun{\text -}part \rangle \langle verb{\text -}part \rangle </mrow>
  <mrow> \langle noun{\text -}part \rangle \amp ::=\ \langle article \rangle \langle noun \rangle [ who \langle verb{\text -}part \rangle ]\dots </mrow>
  <mrow> \langle verb{\text -}part \rangle \amp ::=\ \langle intransitive{\text -}verb \rangle \BNFALT\ ( \langle transitive{\text -}verb \rangle \langle noun{\text -}part \rangle ) </mrow>
  <mrow> \langle article \rangle \amp ::=\ the \BNFALT\ a </mrow>
  <mrow> \langle noun \rangle \amp ::=\ man \BNFALT\ woman \BNFALT\ dog  \BNFALT\ cat  \BNFALT\ computer </mrow>
  <mrow> \langle intransitive{\text -}verb \rangle \amp ::=\ runs \BNFALT\ jumps \BNFALT\ hides </mrow>
  <mrow> \langle transitive{\text -}verb \rangle \amp ::=\ knows \BNFALT\ loves \BNFALT\ chases  \BNFALT\ owns </mrow>
</md>

This grammar can generate sentences such as <q>A dog chases the cat and
the cat hides</q> and <q>The man loves a woman who runs.</q>
The second sentence, for example, is generated by the derivation
<md>
  <mrow> \langle sentence \rangle\ \amp \YIELDS\ \langle simple{\text -}sentence \rangle </mrow>
  <mrow> \amp \YIELDS\ \langle noun{\text -}part \rangle\ \langle verb{\text -}part \rangle</mrow>
  <mrow> \amp \YIELDS\ \langle article \rangle\ \langle noun \rangle\ \langle verb{\text -}part \rangle </mrow>
  <mrow> \amp \YIELDS\ \mbox{the}\ \langle noun \rangle\ \langle verb{\text -}part \rangle</mrow>
  <mrow> \amp \YIELDS\ \mbox{the}\ \mbox{man}\ \langle verb{\text -}part \rangle </mrow>
  <mrow> \amp \YIELDS\ \mbox{the}\ \mbox{man}\ \langle transitive{\text -}verb \rangle\ \langle noun{\text -}part \rangle</mrow>
  <mrow> \amp \YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \langle noun{\text -}part \rangle </mrow>
  <mrow> \amp \YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \langle article \rangle
              \ \langle noun \rangle\ \mbox{who}\ \langle verb{\text -}part \rangle </mrow>
  <mrow> \amp \YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \mbox{a}
              \ \langle noun \rangle\ \mbox{who}\ \langle verb{\text -}part \rangle</mrow>
  <mrow> \amp \YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \mbox{a}
              \ \mbox{woman}\ \mbox{who}\ \langle verb{\text -}part \rangle</mrow>
  <mrow> \amp \YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \mbox{a}
              \ \mbox{woman}\ \mbox{who}\ \langle intransitive{\text -}verb \rangle</mrow>
  <mrow> \amp \YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \mbox{a}
              \ \mbox{woman}\ \mbox{who}\ \mbox{runs}</mrow>
</md>
</p>
  
</example>
<p>
BNF is most often used to specify the syntax of programming languages.
Most programming languages are not, in fact, context-free languages, and
BNF is not capable of expressing all aspects of their syntax.
For example, BNF cannot express the fact that a variable must
be declared before it is used or the fact that the number of
actual parameters in a subroutine call statement must match the number
of formal parameters in the declaration of the subroutine.
So BNF is used to express the context-free aspects of the syntax
of a programming language, and other restrictions on the syntax---such
as the rule about declaring a variable before it is used---are expressed
using informal English descriptions.
</p>
<p>When BNF is applied to programming languages, the terminal symbols
are generally <q>tokens,</q> which are the minimal meaningful units in
a program.  For example, the pair of symbols <m>\lt =</m> constitute a
single token, as does a string such as <c>"Hello World"</c>.
Every number is represented by a single token.  (The actual value
of the number is stored as a so-called <q>attribute</q> of the token,
but the value plays no role in the context-free syntax of the
language.) I will use the
symbol <m>\textbf{number}</m> to represent a numerical token.
Similarly, every variable name, subroutine name, or other identifier
in the program is represented by the same token, which I will denote
as <m>\textbf{ident}</m>.  One final complication:  Some symbols
used in programs, such as <q>]</q> and <q>(</q>, are also used with
a special meaning in BNF grammars.  When such a symbol occurs as
a terminal symbol, I will enclose it in double quotes.  For
example, in the BNF production rule
<me>
  \langle array{\text -}reference \rangle ::=\ 
   \textbf{ident } \text{"[" } \langle expression \rangle \text{ "]"} 
</me>
the <q><m>[</m></q> and <q><m>]</m></q> are terminal symbols in the language
that is being described, rather than the BNF notation for an
optional item.  With this notation, here is part of a BNF
grammar that describes statements in the Java programming
language:

<md>
  <mrow>\langle statement \rangle \amp ::=\ \langle block{\text -}statement \rangle \BNFALT\ \langle if{\text -}statement \rangle </mrow>
   <mrow> \amp \,\BNFALT\ \langle while{\text -}statement\rangle  \BNFALT\ \langle assignment{\text -}statement \rangle</mrow>
  <mrow> \amp \,  \BNFALT\ \langle null{\text -}statement \rangle
</mrow>
<mrow> \langle block{\text -}statement \rangle\amp ::=\ \{ [ \langle statement \rangle ]\dots\ \}</mrow>
<mrow> \langle if{\text -}statement \rangle \amp ::=\ if \text{"("} \langle condition \rangle \text{")"}  \langle statement \rangle [ else \langle statement \rangle~] </mrow>
<mrow> \langle while{\text -}statement \rangle \amp ::=\ while \text{"("}  \langle condition \rangle \text{")"} \langle statement \rangle </mrow>
<mrow> \langle assignment{\text -}statement \rangle \amp  ::=\ \langle variable \rangle = \langle expression \rangle ; </mrow>
<mrow> \langle null{\text -}statement \rangle\amp ::=\ \EMPTYSTRING </mrow>
</md>
The non-terminals <m>\langle condition \rangle</m>, <m>\langle variable \rangle</m>, and 
<m>\langle expression \rangle</m> would, of course, have to be defined by other
production rules in the grammar.  Here is a set of rules that
define simple expressions, made up of numbers, identifiers,
parentheses and the arithmetic operators <m> +, -, *</m> and <m>/</m>:

<md>
  <mrow>\langle expression \rangle \amp ::=\ \langle term \rangle [ \,[ + | - ] \langle term \rangle ]\dots</mrow>
  <mrow>\langle term \rangle \amp ::=\ \langle factor \rangle [ \, [ * | / ] \langle factor \rangle ]\dots </mrow>
  <mrow>\langle factor \rangle  \amp ::=\ \textbf{ident} | \textbf{number} | \text{"}(\text{"}\langle expression \rangle \text{"})\text{"}</mrow>
</md>
The first rule says that an <m>\langle expression \rangle</m> is a sequence of
one or more <m>\langle term \rangle</m>'s, separated by plus or minus signs.
The second rule defines a <m>\langle term \rangle</m> to be a sequence of one or more
<m>\langle factors \rangle</m>, separated by multiplication or division operators.
The last rule says that a <m>\langle factor \rangle</m> can be either an identifier
or a number or an <m>\langle expression \rangle</m> enclosed in parentheses.
This small BNF grammar can generate expressions 
such as <q><m>3*5</m></q> and <q><m> x*(x+1) - 3/(z+2*(3-x)) + 7</m></q>.
The latter expression is made up of three terms: <m>x*(x+1)</m>,
<m>3/(z+2*(3-x))</m>, and <m>7</m>.  The first of these terms is made
up of two factors, <m>x</m> and <m>(x+1)</m>.  The factor <m>(x+1)</m> consists
of the expression <m>x+1</m> inside a pair of parentheses.</p>

<p>The nice thing about this grammar is that the precedence rules
for the operators are implicit in the grammar.  For example, according
to the grammar, the expression <m>3+5*7</m> is seen as <m>\langle term \rangle</m> + <m>\langle term \rangle</m>
where the first term is <m>3</m> and the second term is <m>5*7</m>.
The <m>5*7</m> occurs as a group, which must be evaluated before the
result is added to <m>3</m>.  Parentheses can change the order of
evaluation.  For example, <m>(3+5)*7</m> is generated by the grammar
as a single <m>\langle term \rangle</m> of the form <m><m>\langle factor \rangle * \langle factor \rangle</m></m>.
The first <m>\langle factor \rangle</m> is <m>(3+5)</m>.  When <m>(3+5)*7</m> is evaluated,
the value of <m>(3+5)</m> is computed first and then multiplied
by <m>7</m>.  This is an example of how a grammar that describes
the syntax of a language can also reflect its meaning.</p>



<p>Although this section has not introduced any really new ideas
or theoretical results, I hope it has demonstrated how 
context-free grammars can be applied in practice. </p> 

 </subsection>  
<!-- STOPPED HERE END OF BNF Exercises to go at end  -->
</section>
